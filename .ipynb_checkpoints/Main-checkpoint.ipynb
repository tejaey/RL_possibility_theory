{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fc043b-442b-4e67-9c0a-4c5f2b675f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "MAX_EPISODES = 500\n",
    "MAX_STEPS = 200           \n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99               \n",
    "LR = 1e-3                  \n",
    "EPS_START = 1.0            \n",
    "EPS_END = 0.01            \n",
    "EPS_DECAY = 0.995         \n",
    "TARGET_UPDATE_FREQ = 10     \n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP that takes the state as input and outputs Q-values for each action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer to store tuples of (state, action, reward, next_state, done).\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "                np.array(actions, dtype=np.int64),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.uint8))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def select_action(state, q_network, epsilon, env):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_t)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "def compute_td_loss(batch, q_network, target_network, optimizer):\n",
    "    \"\"\"\n",
    "    Compute the temporal difference loss using a minibatch.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    states_t = torch.FloatTensor(states)\n",
    "    actions_t = torch.LongTensor(actions).unsqueeze(-1)\n",
    "    rewards_t = torch.FloatTensor(rewards).unsqueeze(-1)\n",
    "    next_states_t = torch.FloatTensor(next_states)\n",
    "    dones_t = torch.FloatTensor(dones).unsqueeze(-1)  # 0 or 1\n",
    "\n",
    "    # Get current Q-values\n",
    "    current_q_values = q_network(states_t).gather(1, actions_t)\n",
    "\n",
    "    # Get next Q-values from the target network\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states_t).max(dim=1, keepdim=True)[0]\n",
    "\n",
    "    # If done, next_q_values should be 0\n",
    "    expected_q_values = rewards_t + GAMMA * next_q_values * (1 - dones_t)\n",
    "\n",
    "    loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27614a38-5f56-4fc5-a5c1-beea3f8f75a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Reward: 18.0, Epsilon: 0.995\n",
      "Episode 2, Reward: 17.0, Epsilon: 0.990\n",
      "Episode 3, Reward: 32.0, Epsilon: 0.985\n",
      "Episode 4, Reward: 25.0, Epsilon: 0.980\n",
      "Episode 5, Reward: 20.0, Epsilon: 0.975\n",
      "Episode 6, Reward: 43.0, Epsilon: 0.970\n",
      "Episode 7, Reward: 24.0, Epsilon: 0.966\n",
      "Episode 8, Reward: 13.0, Epsilon: 0.961\n",
      "Episode 9, Reward: 18.0, Epsilon: 0.956\n",
      "Episode 10, Reward: 17.0, Epsilon: 0.951\n",
      "Episode 11, Reward: 12.0, Epsilon: 0.946\n",
      "Episode 12, Reward: 16.0, Epsilon: 0.942\n",
      "Episode 13, Reward: 45.0, Epsilon: 0.937\n",
      "Episode 14, Reward: 12.0, Epsilon: 0.932\n",
      "Episode 15, Reward: 37.0, Epsilon: 0.928\n",
      "Episode 16, Reward: 21.0, Epsilon: 0.923\n",
      "Episode 17, Reward: 23.0, Epsilon: 0.918\n",
      "Episode 18, Reward: 10.0, Epsilon: 0.914\n",
      "Episode 19, Reward: 20.0, Epsilon: 0.909\n",
      "Episode 20, Reward: 19.0, Epsilon: 0.905\n",
      "Episode 21, Reward: 33.0, Epsilon: 0.900\n",
      "Episode 22, Reward: 39.0, Epsilon: 0.896\n",
      "Episode 23, Reward: 36.0, Epsilon: 0.891\n",
      "Episode 24, Reward: 24.0, Epsilon: 0.887\n",
      "Episode 25, Reward: 33.0, Epsilon: 0.882\n",
      "Episode 26, Reward: 13.0, Epsilon: 0.878\n",
      "Episode 27, Reward: 20.0, Epsilon: 0.873\n",
      "Episode 28, Reward: 35.0, Epsilon: 0.869\n",
      "Episode 29, Reward: 16.0, Epsilon: 0.865\n",
      "Episode 30, Reward: 50.0, Epsilon: 0.860\n",
      "Episode 31, Reward: 45.0, Epsilon: 0.856\n",
      "Episode 32, Reward: 26.0, Epsilon: 0.852\n",
      "Episode 33, Reward: 34.0, Epsilon: 0.848\n",
      "Episode 34, Reward: 44.0, Epsilon: 0.843\n",
      "Episode 35, Reward: 21.0, Epsilon: 0.839\n",
      "Episode 36, Reward: 59.0, Epsilon: 0.835\n",
      "Episode 37, Reward: 36.0, Epsilon: 0.831\n",
      "Episode 38, Reward: 11.0, Epsilon: 0.827\n",
      "Episode 39, Reward: 47.0, Epsilon: 0.822\n",
      "Episode 40, Reward: 13.0, Epsilon: 0.818\n",
      "Episode 41, Reward: 17.0, Epsilon: 0.814\n",
      "Episode 42, Reward: 42.0, Epsilon: 0.810\n",
      "Episode 43, Reward: 77.0, Epsilon: 0.806\n",
      "Episode 44, Reward: 8.0, Epsilon: 0.802\n",
      "Episode 45, Reward: 41.0, Epsilon: 0.798\n",
      "Episode 46, Reward: 32.0, Epsilon: 0.794\n",
      "Episode 47, Reward: 14.0, Epsilon: 0.790\n",
      "Episode 48, Reward: 11.0, Epsilon: 0.786\n",
      "Episode 49, Reward: 31.0, Epsilon: 0.782\n",
      "Episode 50, Reward: 19.0, Epsilon: 0.778\n",
      "Episode 51, Reward: 30.0, Epsilon: 0.774\n",
      "Episode 52, Reward: 27.0, Epsilon: 0.771\n",
      "Episode 53, Reward: 34.0, Epsilon: 0.767\n",
      "Episode 54, Reward: 41.0, Epsilon: 0.763\n",
      "Episode 55, Reward: 44.0, Epsilon: 0.759\n",
      "Episode 56, Reward: 46.0, Epsilon: 0.755\n",
      "Episode 57, Reward: 29.0, Epsilon: 0.751\n",
      "Episode 58, Reward: 12.0, Epsilon: 0.748\n",
      "Episode 59, Reward: 30.0, Epsilon: 0.744\n",
      "Episode 60, Reward: 26.0, Epsilon: 0.740\n",
      "Episode 61, Reward: 17.0, Epsilon: 0.737\n",
      "Episode 62, Reward: 30.0, Epsilon: 0.733\n",
      "Episode 63, Reward: 46.0, Epsilon: 0.729\n",
      "Episode 64, Reward: 26.0, Epsilon: 0.726\n",
      "Episode 65, Reward: 36.0, Epsilon: 0.722\n",
      "Episode 66, Reward: 73.0, Epsilon: 0.718\n",
      "Episode 67, Reward: 45.0, Epsilon: 0.715\n",
      "Episode 68, Reward: 47.0, Epsilon: 0.711\n",
      "Episode 69, Reward: 74.0, Epsilon: 0.708\n",
      "Episode 70, Reward: 11.0, Epsilon: 0.704\n",
      "Episode 71, Reward: 53.0, Epsilon: 0.701\n",
      "Episode 72, Reward: 67.0, Epsilon: 0.697\n",
      "Episode 73, Reward: 70.0, Epsilon: 0.694\n",
      "Episode 74, Reward: 79.0, Epsilon: 0.690\n",
      "Episode 75, Reward: 15.0, Epsilon: 0.687\n",
      "Episode 76, Reward: 48.0, Epsilon: 0.683\n",
      "Episode 77, Reward: 56.0, Epsilon: 0.680\n",
      "Episode 78, Reward: 38.0, Epsilon: 0.676\n",
      "Episode 79, Reward: 26.0, Epsilon: 0.673\n",
      "Episode 80, Reward: 48.0, Epsilon: 0.670\n",
      "Episode 81, Reward: 104.0, Epsilon: 0.666\n",
      "Episode 82, Reward: 28.0, Epsilon: 0.663\n",
      "Episode 83, Reward: 59.0, Epsilon: 0.660\n",
      "Episode 84, Reward: 17.0, Epsilon: 0.656\n",
      "Episode 85, Reward: 17.0, Epsilon: 0.653\n",
      "Episode 86, Reward: 19.0, Epsilon: 0.650\n",
      "Episode 87, Reward: 96.0, Epsilon: 0.647\n",
      "Episode 88, Reward: 29.0, Epsilon: 0.643\n",
      "Episode 89, Reward: 22.0, Epsilon: 0.640\n",
      "Episode 90, Reward: 49.0, Epsilon: 0.637\n",
      "Episode 91, Reward: 59.0, Epsilon: 0.634\n",
      "Episode 92, Reward: 30.0, Epsilon: 0.631\n",
      "Episode 93, Reward: 117.0, Epsilon: 0.627\n",
      "Episode 94, Reward: 39.0, Epsilon: 0.624\n",
      "Episode 95, Reward: 46.0, Epsilon: 0.621\n",
      "Episode 96, Reward: 45.0, Epsilon: 0.618\n",
      "Episode 97, Reward: 27.0, Epsilon: 0.615\n",
      "Episode 98, Reward: 80.0, Epsilon: 0.612\n",
      "Episode 99, Reward: 17.0, Epsilon: 0.609\n",
      "Episode 100, Reward: 105.0, Epsilon: 0.606\n",
      "Episode 101, Reward: 74.0, Epsilon: 0.603\n",
      "Episode 102, Reward: 106.0, Epsilon: 0.600\n",
      "Episode 103, Reward: 12.0, Epsilon: 0.597\n",
      "Episode 104, Reward: 74.0, Epsilon: 0.594\n",
      "Episode 105, Reward: 39.0, Epsilon: 0.591\n",
      "Episode 106, Reward: 86.0, Epsilon: 0.588\n",
      "Episode 107, Reward: 20.0, Epsilon: 0.585\n",
      "Episode 108, Reward: 12.0, Epsilon: 0.582\n",
      "Episode 109, Reward: 17.0, Epsilon: 0.579\n",
      "Episode 110, Reward: 187.0, Epsilon: 0.576\n",
      "Episode 111, Reward: 105.0, Epsilon: 0.573\n",
      "Episode 112, Reward: 68.0, Epsilon: 0.570\n",
      "Episode 113, Reward: 118.0, Epsilon: 0.568\n",
      "Episode 114, Reward: 65.0, Epsilon: 0.565\n",
      "Episode 115, Reward: 144.0, Epsilon: 0.562\n",
      "Episode 116, Reward: 26.0, Epsilon: 0.559\n",
      "Episode 117, Reward: 64.0, Epsilon: 0.556\n",
      "Episode 118, Reward: 57.0, Epsilon: 0.554\n",
      "Episode 119, Reward: 11.0, Epsilon: 0.551\n",
      "Episode 120, Reward: 19.0, Epsilon: 0.548\n",
      "Episode 121, Reward: 95.0, Epsilon: 0.545\n",
      "Episode 122, Reward: 55.0, Epsilon: 0.543\n",
      "Episode 123, Reward: 47.0, Epsilon: 0.540\n",
      "Episode 124, Reward: 101.0, Epsilon: 0.537\n",
      "Episode 125, Reward: 84.0, Epsilon: 0.534\n",
      "Episode 126, Reward: 100.0, Epsilon: 0.532\n",
      "Episode 127, Reward: 78.0, Epsilon: 0.529\n",
      "Episode 128, Reward: 12.0, Epsilon: 0.526\n",
      "Episode 129, Reward: 146.0, Epsilon: 0.524\n",
      "Episode 130, Reward: 26.0, Epsilon: 0.521\n",
      "Episode 131, Reward: 18.0, Epsilon: 0.519\n",
      "Episode 132, Reward: 97.0, Epsilon: 0.516\n",
      "Episode 133, Reward: 125.0, Epsilon: 0.513\n",
      "Episode 134, Reward: 153.0, Epsilon: 0.511\n",
      "Episode 135, Reward: 84.0, Epsilon: 0.508\n",
      "Episode 136, Reward: 94.0, Epsilon: 0.506\n",
      "Episode 137, Reward: 81.0, Epsilon: 0.503\n",
      "Episode 138, Reward: 76.0, Epsilon: 0.501\n",
      "Episode 139, Reward: 82.0, Epsilon: 0.498\n",
      "Episode 140, Reward: 132.0, Epsilon: 0.496\n",
      "Episode 141, Reward: 14.0, Epsilon: 0.493\n",
      "Episode 142, Reward: 76.0, Epsilon: 0.491\n",
      "Episode 143, Reward: 95.0, Epsilon: 0.488\n",
      "Episode 144, Reward: 118.0, Epsilon: 0.486\n",
      "Episode 145, Reward: 163.0, Epsilon: 0.483\n",
      "Episode 146, Reward: 113.0, Epsilon: 0.481\n",
      "Episode 147, Reward: 170.0, Epsilon: 0.479\n",
      "Episode 148, Reward: 19.0, Epsilon: 0.476\n",
      "Episode 149, Reward: 12.0, Epsilon: 0.474\n",
      "Episode 150, Reward: 107.0, Epsilon: 0.471\n",
      "Episode 151, Reward: 16.0, Epsilon: 0.469\n",
      "Episode 152, Reward: 58.0, Epsilon: 0.467\n",
      "Episode 153, Reward: 47.0, Epsilon: 0.464\n",
      "Episode 154, Reward: 123.0, Epsilon: 0.462\n",
      "Episode 155, Reward: 102.0, Epsilon: 0.460\n",
      "Episode 156, Reward: 50.0, Epsilon: 0.458\n",
      "Episode 157, Reward: 124.0, Epsilon: 0.455\n",
      "Episode 158, Reward: 12.0, Epsilon: 0.453\n",
      "Episode 159, Reward: 27.0, Epsilon: 0.451\n",
      "Episode 160, Reward: 23.0, Epsilon: 0.448\n",
      "Episode 161, Reward: 69.0, Epsilon: 0.446\n",
      "Episode 162, Reward: 116.0, Epsilon: 0.444\n",
      "Episode 163, Reward: 11.0, Epsilon: 0.442\n",
      "Episode 164, Reward: 33.0, Epsilon: 0.440\n",
      "Episode 165, Reward: 44.0, Epsilon: 0.437\n",
      "Episode 166, Reward: 19.0, Epsilon: 0.435\n",
      "Episode 167, Reward: 112.0, Epsilon: 0.433\n",
      "Episode 168, Reward: 119.0, Epsilon: 0.431\n",
      "Episode 169, Reward: 12.0, Epsilon: 0.429\n",
      "Episode 170, Reward: 11.0, Epsilon: 0.427\n",
      "Episode 171, Reward: 139.0, Epsilon: 0.424\n",
      "Episode 172, Reward: 31.0, Epsilon: 0.422\n",
      "Episode 173, Reward: 20.0, Epsilon: 0.420\n",
      "Episode 174, Reward: 112.0, Epsilon: 0.418\n",
      "Episode 175, Reward: 142.0, Epsilon: 0.416\n",
      "Episode 176, Reward: 9.0, Epsilon: 0.414\n",
      "Episode 177, Reward: 122.0, Epsilon: 0.412\n",
      "Episode 178, Reward: 13.0, Epsilon: 0.410\n",
      "Episode 179, Reward: 85.0, Epsilon: 0.408\n",
      "Episode 180, Reward: 13.0, Epsilon: 0.406\n",
      "Episode 181, Reward: 13.0, Epsilon: 0.404\n",
      "Episode 182, Reward: 23.0, Epsilon: 0.402\n",
      "Episode 183, Reward: 29.0, Epsilon: 0.400\n",
      "Episode 184, Reward: 99.0, Epsilon: 0.398\n",
      "Episode 185, Reward: 92.0, Epsilon: 0.396\n",
      "Episode 186, Reward: 117.0, Epsilon: 0.394\n",
      "Episode 187, Reward: 115.0, Epsilon: 0.392\n",
      "Episode 188, Reward: 22.0, Epsilon: 0.390\n",
      "Episode 189, Reward: 35.0, Epsilon: 0.388\n",
      "Episode 190, Reward: 20.0, Epsilon: 0.386\n",
      "Episode 191, Reward: 92.0, Epsilon: 0.384\n",
      "Episode 192, Reward: 16.0, Epsilon: 0.382\n",
      "Episode 193, Reward: 139.0, Epsilon: 0.380\n",
      "Episode 194, Reward: 26.0, Epsilon: 0.378\n",
      "Episode 195, Reward: 10.0, Epsilon: 0.376\n",
      "Episode 196, Reward: 138.0, Epsilon: 0.374\n",
      "Episode 197, Reward: 104.0, Epsilon: 0.373\n",
      "Episode 198, Reward: 141.0, Epsilon: 0.371\n",
      "Episode 199, Reward: 103.0, Epsilon: 0.369\n",
      "Episode 200, Reward: 106.0, Epsilon: 0.367\n",
      "Episode 201, Reward: 174.0, Epsilon: 0.365\n",
      "Episode 202, Reward: 50.0, Epsilon: 0.363\n",
      "Episode 203, Reward: 23.0, Epsilon: 0.361\n",
      "Episode 204, Reward: 108.0, Epsilon: 0.360\n",
      "Episode 205, Reward: 125.0, Epsilon: 0.358\n",
      "Episode 206, Reward: 14.0, Epsilon: 0.356\n",
      "Episode 207, Reward: 39.0, Epsilon: 0.354\n",
      "Episode 208, Reward: 16.0, Epsilon: 0.353\n",
      "Episode 209, Reward: 88.0, Epsilon: 0.351\n",
      "Episode 210, Reward: 51.0, Epsilon: 0.349\n",
      "Episode 211, Reward: 13.0, Epsilon: 0.347\n",
      "Episode 212, Reward: 19.0, Epsilon: 0.346\n",
      "Episode 213, Reward: 16.0, Epsilon: 0.344\n",
      "Episode 214, Reward: 22.0, Epsilon: 0.342\n",
      "Episode 215, Reward: 93.0, Epsilon: 0.340\n",
      "Episode 216, Reward: 21.0, Epsilon: 0.339\n",
      "Episode 217, Reward: 24.0, Epsilon: 0.337\n",
      "Episode 218, Reward: 76.0, Epsilon: 0.335\n",
      "Episode 219, Reward: 132.0, Epsilon: 0.334\n",
      "Episode 220, Reward: 91.0, Epsilon: 0.332\n",
      "Episode 221, Reward: 16.0, Epsilon: 0.330\n",
      "Episode 222, Reward: 134.0, Epsilon: 0.329\n",
      "Episode 223, Reward: 131.0, Epsilon: 0.327\n",
      "Episode 224, Reward: 103.0, Epsilon: 0.325\n",
      "Episode 225, Reward: 97.0, Epsilon: 0.324\n",
      "Episode 226, Reward: 127.0, Epsilon: 0.322\n",
      "Episode 227, Reward: 122.0, Epsilon: 0.321\n",
      "Episode 228, Reward: 48.0, Epsilon: 0.319\n",
      "Episode 229, Reward: 101.0, Epsilon: 0.317\n",
      "Episode 230, Reward: 47.0, Epsilon: 0.316\n",
      "Episode 231, Reward: 181.0, Epsilon: 0.314\n",
      "Episode 232, Reward: 24.0, Epsilon: 0.313\n",
      "Episode 233, Reward: 14.0, Epsilon: 0.311\n",
      "Episode 234, Reward: 57.0, Epsilon: 0.309\n",
      "Episode 235, Reward: 98.0, Epsilon: 0.308\n",
      "Episode 236, Reward: 36.0, Epsilon: 0.306\n",
      "Episode 237, Reward: 77.0, Epsilon: 0.305\n",
      "Episode 238, Reward: 198.0, Epsilon: 0.303\n",
      "Episode 239, Reward: 22.0, Epsilon: 0.302\n",
      "Episode 240, Reward: 101.0, Epsilon: 0.300\n",
      "Episode 241, Reward: 85.0, Epsilon: 0.299\n",
      "Episode 242, Reward: 40.0, Epsilon: 0.297\n",
      "Episode 243, Reward: 12.0, Epsilon: 0.296\n",
      "Episode 244, Reward: 100.0, Epsilon: 0.294\n",
      "Episode 245, Reward: 65.0, Epsilon: 0.293\n",
      "Episode 246, Reward: 12.0, Epsilon: 0.291\n",
      "Episode 247, Reward: 71.0, Epsilon: 0.290\n",
      "Episode 248, Reward: 89.0, Epsilon: 0.288\n",
      "Episode 249, Reward: 96.0, Epsilon: 0.287\n",
      "Episode 250, Reward: 99.0, Epsilon: 0.286\n",
      "Episode 251, Reward: 14.0, Epsilon: 0.284\n",
      "Episode 252, Reward: 14.0, Epsilon: 0.283\n",
      "Episode 253, Reward: 28.0, Epsilon: 0.281\n",
      "Episode 254, Reward: 113.0, Epsilon: 0.280\n",
      "Episode 255, Reward: 105.0, Epsilon: 0.279\n",
      "Episode 256, Reward: 29.0, Epsilon: 0.277\n",
      "Episode 257, Reward: 88.0, Epsilon: 0.276\n",
      "Episode 258, Reward: 125.0, Epsilon: 0.274\n",
      "Episode 259, Reward: 102.0, Epsilon: 0.273\n",
      "Episode 260, Reward: 65.0, Epsilon: 0.272\n",
      "Episode 261, Reward: 17.0, Epsilon: 0.270\n",
      "Episode 262, Reward: 134.0, Epsilon: 0.269\n",
      "Episode 263, Reward: 41.0, Epsilon: 0.268\n",
      "Episode 264, Reward: 102.0, Epsilon: 0.266\n",
      "Episode 265, Reward: 53.0, Epsilon: 0.265\n",
      "Episode 266, Reward: 16.0, Epsilon: 0.264\n",
      "Episode 267, Reward: 20.0, Epsilon: 0.262\n",
      "Episode 268, Reward: 45.0, Epsilon: 0.261\n",
      "Episode 269, Reward: 94.0, Epsilon: 0.260\n",
      "Episode 270, Reward: 42.0, Epsilon: 0.258\n",
      "Episode 271, Reward: 13.0, Epsilon: 0.257\n",
      "Episode 272, Reward: 103.0, Epsilon: 0.256\n",
      "Episode 273, Reward: 109.0, Epsilon: 0.255\n",
      "Episode 274, Reward: 16.0, Epsilon: 0.253\n",
      "Episode 275, Reward: 61.0, Epsilon: 0.252\n",
      "Episode 276, Reward: 102.0, Epsilon: 0.251\n",
      "Episode 277, Reward: 16.0, Epsilon: 0.249\n",
      "Episode 278, Reward: 37.0, Epsilon: 0.248\n",
      "Episode 279, Reward: 14.0, Epsilon: 0.247\n",
      "Episode 280, Reward: 66.0, Epsilon: 0.246\n",
      "Episode 281, Reward: 100.0, Epsilon: 0.245\n",
      "Episode 282, Reward: 98.0, Epsilon: 0.243\n",
      "Episode 283, Reward: 29.0, Epsilon: 0.242\n",
      "Episode 284, Reward: 36.0, Epsilon: 0.241\n",
      "Episode 285, Reward: 32.0, Epsilon: 0.240\n",
      "Episode 286, Reward: 34.0, Epsilon: 0.238\n",
      "Episode 287, Reward: 13.0, Epsilon: 0.237\n",
      "Episode 288, Reward: 18.0, Epsilon: 0.236\n",
      "Episode 289, Reward: 102.0, Epsilon: 0.235\n",
      "Episode 290, Reward: 23.0, Epsilon: 0.234\n",
      "Episode 291, Reward: 28.0, Epsilon: 0.233\n",
      "Episode 292, Reward: 177.0, Epsilon: 0.231\n",
      "Episode 293, Reward: 113.0, Epsilon: 0.230\n",
      "Episode 294, Reward: 35.0, Epsilon: 0.229\n",
      "Episode 295, Reward: 43.0, Epsilon: 0.228\n",
      "Episode 296, Reward: 105.0, Epsilon: 0.227\n",
      "Episode 297, Reward: 102.0, Epsilon: 0.226\n",
      "Episode 298, Reward: 100.0, Epsilon: 0.225\n",
      "Episode 299, Reward: 14.0, Epsilon: 0.223\n",
      "Episode 300, Reward: 109.0, Epsilon: 0.222\n",
      "Episode 301, Reward: 95.0, Epsilon: 0.221\n",
      "Episode 302, Reward: 76.0, Epsilon: 0.220\n",
      "Episode 303, Reward: 28.0, Epsilon: 0.219\n",
      "Episode 304, Reward: 17.0, Epsilon: 0.218\n",
      "Episode 305, Reward: 30.0, Epsilon: 0.217\n",
      "Episode 306, Reward: 30.0, Epsilon: 0.216\n",
      "Episode 307, Reward: 103.0, Epsilon: 0.215\n",
      "Episode 308, Reward: 106.0, Epsilon: 0.214\n",
      "Episode 309, Reward: 38.0, Epsilon: 0.212\n",
      "Episode 310, Reward: 116.0, Epsilon: 0.211\n",
      "Episode 311, Reward: 40.0, Epsilon: 0.210\n",
      "Episode 312, Reward: 141.0, Epsilon: 0.209\n",
      "Episode 313, Reward: 103.0, Epsilon: 0.208\n",
      "Episode 314, Reward: 25.0, Epsilon: 0.207\n",
      "Episode 315, Reward: 42.0, Epsilon: 0.206\n",
      "Episode 316, Reward: 110.0, Epsilon: 0.205\n",
      "Episode 317, Reward: 103.0, Epsilon: 0.204\n",
      "Episode 318, Reward: 33.0, Epsilon: 0.203\n",
      "Episode 319, Reward: 114.0, Epsilon: 0.202\n",
      "Episode 320, Reward: 20.0, Epsilon: 0.201\n",
      "Episode 321, Reward: 18.0, Epsilon: 0.200\n",
      "Episode 322, Reward: 123.0, Epsilon: 0.199\n",
      "Episode 323, Reward: 51.0, Epsilon: 0.198\n",
      "Episode 324, Reward: 126.0, Epsilon: 0.197\n",
      "Episode 325, Reward: 101.0, Epsilon: 0.196\n",
      "Episode 326, Reward: 101.0, Epsilon: 0.195\n",
      "Episode 327, Reward: 112.0, Epsilon: 0.194\n",
      "Episode 328, Reward: 101.0, Epsilon: 0.193\n",
      "Episode 329, Reward: 116.0, Epsilon: 0.192\n",
      "Episode 330, Reward: 117.0, Epsilon: 0.191\n",
      "Episode 331, Reward: 111.0, Epsilon: 0.190\n",
      "Episode 332, Reward: 80.0, Epsilon: 0.189\n",
      "Episode 333, Reward: 108.0, Epsilon: 0.188\n",
      "Episode 334, Reward: 112.0, Epsilon: 0.187\n",
      "Episode 335, Reward: 74.0, Epsilon: 0.187\n",
      "Episode 336, Reward: 10.0, Epsilon: 0.186\n",
      "Episode 337, Reward: 104.0, Epsilon: 0.185\n",
      "Episode 338, Reward: 110.0, Epsilon: 0.184\n",
      "Episode 339, Reward: 115.0, Epsilon: 0.183\n",
      "Episode 340, Reward: 110.0, Epsilon: 0.182\n",
      "Episode 341, Reward: 123.0, Epsilon: 0.181\n",
      "Episode 342, Reward: 107.0, Epsilon: 0.180\n",
      "Episode 343, Reward: 32.0, Epsilon: 0.179\n",
      "Episode 344, Reward: 113.0, Epsilon: 0.178\n",
      "Episode 345, Reward: 123.0, Epsilon: 0.177\n",
      "Episode 346, Reward: 113.0, Epsilon: 0.177\n",
      "Episode 347, Reward: 108.0, Epsilon: 0.176\n",
      "Episode 348, Reward: 43.0, Epsilon: 0.175\n",
      "Episode 349, Reward: 118.0, Epsilon: 0.174\n",
      "Episode 350, Reward: 116.0, Epsilon: 0.173\n",
      "Episode 351, Reward: 200.0, Epsilon: 0.172\n",
      "Episode 352, Reward: 112.0, Epsilon: 0.171\n",
      "Episode 353, Reward: 144.0, Epsilon: 0.170\n",
      "Episode 354, Reward: 140.0, Epsilon: 0.170\n",
      "Episode 355, Reward: 136.0, Epsilon: 0.169\n",
      "Episode 356, Reward: 124.0, Epsilon: 0.168\n",
      "Episode 357, Reward: 121.0, Epsilon: 0.167\n",
      "Episode 358, Reward: 44.0, Epsilon: 0.166\n",
      "Episode 359, Reward: 91.0, Epsilon: 0.165\n",
      "Episode 360, Reward: 29.0, Epsilon: 0.165\n",
      "Episode 361, Reward: 144.0, Epsilon: 0.164\n",
      "Episode 362, Reward: 39.0, Epsilon: 0.163\n",
      "Episode 363, Reward: 19.0, Epsilon: 0.162\n",
      "Episode 364, Reward: 138.0, Epsilon: 0.161\n",
      "Episode 365, Reward: 143.0, Epsilon: 0.160\n",
      "Episode 366, Reward: 127.0, Epsilon: 0.160\n",
      "Episode 367, Reward: 128.0, Epsilon: 0.159\n",
      "Episode 368, Reward: 55.0, Epsilon: 0.158\n",
      "Episode 369, Reward: 45.0, Epsilon: 0.157\n",
      "Episode 370, Reward: 121.0, Epsilon: 0.157\n",
      "Episode 371, Reward: 136.0, Epsilon: 0.156\n",
      "Episode 372, Reward: 135.0, Epsilon: 0.155\n",
      "Episode 373, Reward: 153.0, Epsilon: 0.154\n",
      "Episode 374, Reward: 150.0, Epsilon: 0.153\n",
      "Episode 375, Reward: 120.0, Epsilon: 0.153\n",
      "Episode 376, Reward: 51.0, Epsilon: 0.152\n",
      "Episode 377, Reward: 52.0, Epsilon: 0.151\n",
      "Episode 378, Reward: 139.0, Epsilon: 0.150\n",
      "Episode 379, Reward: 50.0, Epsilon: 0.150\n",
      "Episode 380, Reward: 200.0, Epsilon: 0.149\n",
      "Episode 381, Reward: 37.0, Epsilon: 0.148\n",
      "Episode 382, Reward: 127.0, Epsilon: 0.147\n",
      "Episode 383, Reward: 143.0, Epsilon: 0.147\n",
      "Episode 384, Reward: 39.0, Epsilon: 0.146\n",
      "Episode 385, Reward: 15.0, Epsilon: 0.145\n",
      "Episode 386, Reward: 115.0, Epsilon: 0.144\n",
      "Episode 387, Reward: 122.0, Epsilon: 0.144\n",
      "Episode 388, Reward: 51.0, Epsilon: 0.143\n",
      "Episode 389, Reward: 128.0, Epsilon: 0.142\n",
      "Episode 390, Reward: 32.0, Epsilon: 0.142\n",
      "Episode 391, Reward: 116.0, Epsilon: 0.141\n",
      "Episode 392, Reward: 45.0, Epsilon: 0.140\n",
      "Episode 393, Reward: 137.0, Epsilon: 0.139\n",
      "Episode 394, Reward: 31.0, Epsilon: 0.139\n",
      "Episode 395, Reward: 122.0, Epsilon: 0.138\n",
      "Episode 396, Reward: 121.0, Epsilon: 0.137\n",
      "Episode 397, Reward: 133.0, Epsilon: 0.137\n",
      "Episode 398, Reward: 129.0, Epsilon: 0.136\n",
      "Episode 399, Reward: 42.0, Epsilon: 0.135\n",
      "Episode 400, Reward: 44.0, Epsilon: 0.135\n",
      "Episode 401, Reward: 124.0, Epsilon: 0.134\n",
      "Episode 402, Reward: 136.0, Epsilon: 0.133\n",
      "Episode 403, Reward: 33.0, Epsilon: 0.133\n",
      "Episode 404, Reward: 32.0, Epsilon: 0.132\n",
      "Episode 405, Reward: 173.0, Epsilon: 0.131\n",
      "Episode 406, Reward: 132.0, Epsilon: 0.131\n",
      "Episode 407, Reward: 167.0, Epsilon: 0.130\n",
      "Episode 408, Reward: 142.0, Epsilon: 0.129\n",
      "Episode 409, Reward: 127.0, Epsilon: 0.129\n",
      "Episode 410, Reward: 23.0, Epsilon: 0.128\n",
      "Episode 411, Reward: 151.0, Epsilon: 0.127\n",
      "Episode 412, Reward: 125.0, Epsilon: 0.127\n",
      "Episode 413, Reward: 115.0, Epsilon: 0.126\n",
      "Episode 414, Reward: 144.0, Epsilon: 0.126\n",
      "Episode 415, Reward: 128.0, Epsilon: 0.125\n",
      "Episode 416, Reward: 127.0, Epsilon: 0.124\n",
      "Episode 417, Reward: 110.0, Epsilon: 0.124\n",
      "Episode 418, Reward: 17.0, Epsilon: 0.123\n",
      "Episode 419, Reward: 143.0, Epsilon: 0.122\n",
      "Episode 420, Reward: 122.0, Epsilon: 0.122\n",
      "Episode 421, Reward: 128.0, Epsilon: 0.121\n",
      "Episode 422, Reward: 132.0, Epsilon: 0.121\n",
      "Episode 423, Reward: 109.0, Epsilon: 0.120\n",
      "Episode 424, Reward: 140.0, Epsilon: 0.119\n",
      "Episode 425, Reward: 125.0, Epsilon: 0.119\n",
      "Episode 426, Reward: 111.0, Epsilon: 0.118\n",
      "Episode 427, Reward: 21.0, Epsilon: 0.118\n",
      "Episode 428, Reward: 125.0, Epsilon: 0.117\n",
      "Episode 429, Reward: 120.0, Epsilon: 0.116\n",
      "Episode 430, Reward: 122.0, Epsilon: 0.116\n",
      "Episode 431, Reward: 155.0, Epsilon: 0.115\n",
      "Episode 432, Reward: 119.0, Epsilon: 0.115\n",
      "Episode 433, Reward: 27.0, Epsilon: 0.114\n",
      "Episode 434, Reward: 116.0, Epsilon: 0.114\n",
      "Episode 435, Reward: 113.0, Epsilon: 0.113\n",
      "Episode 436, Reward: 119.0, Epsilon: 0.112\n",
      "Episode 437, Reward: 149.0, Epsilon: 0.112\n",
      "Episode 438, Reward: 140.0, Epsilon: 0.111\n",
      "Episode 439, Reward: 123.0, Epsilon: 0.111\n",
      "Episode 440, Reward: 108.0, Epsilon: 0.110\n",
      "Episode 441, Reward: 35.0, Epsilon: 0.110\n",
      "Episode 442, Reward: 112.0, Epsilon: 0.109\n",
      "Episode 443, Reward: 135.0, Epsilon: 0.109\n",
      "Episode 444, Reward: 141.0, Epsilon: 0.108\n",
      "Episode 445, Reward: 27.0, Epsilon: 0.107\n",
      "Episode 446, Reward: 132.0, Epsilon: 0.107\n",
      "Episode 447, Reward: 188.0, Epsilon: 0.106\n",
      "Episode 448, Reward: 144.0, Epsilon: 0.106\n",
      "Episode 449, Reward: 200.0, Epsilon: 0.105\n",
      "Episode 450, Reward: 14.0, Epsilon: 0.105\n",
      "Episode 451, Reward: 200.0, Epsilon: 0.104\n",
      "Episode 452, Reward: 116.0, Epsilon: 0.104\n",
      "Episode 453, Reward: 18.0, Epsilon: 0.103\n",
      "Episode 454, Reward: 14.0, Epsilon: 0.103\n",
      "Episode 455, Reward: 164.0, Epsilon: 0.102\n",
      "Episode 456, Reward: 108.0, Epsilon: 0.102\n",
      "Episode 457, Reward: 195.0, Epsilon: 0.101\n",
      "Episode 458, Reward: 118.0, Epsilon: 0.101\n",
      "Episode 459, Reward: 13.0, Epsilon: 0.100\n",
      "Episode 460, Reward: 115.0, Epsilon: 0.100\n",
      "Episode 461, Reward: 111.0, Epsilon: 0.099\n",
      "Episode 462, Reward: 108.0, Epsilon: 0.099\n",
      "Episode 463, Reward: 156.0, Epsilon: 0.098\n",
      "Episode 464, Reward: 140.0, Epsilon: 0.098\n",
      "Episode 465, Reward: 136.0, Epsilon: 0.097\n",
      "Episode 466, Reward: 200.0, Epsilon: 0.097\n",
      "Episode 467, Reward: 200.0, Epsilon: 0.096\n",
      "Episode 468, Reward: 147.0, Epsilon: 0.096\n",
      "Episode 469, Reward: 24.0, Epsilon: 0.095\n",
      "Episode 470, Reward: 127.0, Epsilon: 0.095\n",
      "Episode 471, Reward: 57.0, Epsilon: 0.094\n",
      "Episode 472, Reward: 148.0, Epsilon: 0.094\n",
      "Episode 473, Reward: 136.0, Epsilon: 0.093\n",
      "Episode 474, Reward: 122.0, Epsilon: 0.093\n",
      "Episode 475, Reward: 157.0, Epsilon: 0.092\n",
      "Episode 476, Reward: 111.0, Epsilon: 0.092\n",
      "Episode 477, Reward: 166.0, Epsilon: 0.092\n",
      "Episode 478, Reward: 118.0, Epsilon: 0.091\n",
      "Episode 479, Reward: 139.0, Epsilon: 0.091\n",
      "Episode 480, Reward: 134.0, Epsilon: 0.090\n",
      "Episode 481, Reward: 126.0, Epsilon: 0.090\n",
      "Episode 482, Reward: 200.0, Epsilon: 0.089\n",
      "Episode 483, Reward: 173.0, Epsilon: 0.089\n",
      "Episode 484, Reward: 147.0, Epsilon: 0.088\n",
      "Episode 485, Reward: 121.0, Epsilon: 0.088\n",
      "Episode 486, Reward: 136.0, Epsilon: 0.088\n",
      "Episode 487, Reward: 175.0, Epsilon: 0.087\n",
      "Episode 488, Reward: 119.0, Epsilon: 0.087\n",
      "Episode 489, Reward: 151.0, Epsilon: 0.086\n",
      "Episode 490, Reward: 130.0, Epsilon: 0.086\n",
      "Episode 491, Reward: 48.0, Epsilon: 0.085\n",
      "Episode 492, Reward: 169.0, Epsilon: 0.085\n",
      "Episode 493, Reward: 200.0, Epsilon: 0.084\n",
      "Episode 494, Reward: 200.0, Epsilon: 0.084\n",
      "Episode 495, Reward: 29.0, Epsilon: 0.084\n",
      "Episode 496, Reward: 35.0, Epsilon: 0.083\n",
      "Episode 497, Reward: 129.0, Epsilon: 0.083\n",
      "Episode 498, Reward: 89.0, Epsilon: 0.082\n",
      "Episode 499, Reward: 17.0, Epsilon: 0.082\n",
      "Episode 500, Reward: 116.0, Epsilon: 0.082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize Q-network and target network\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "target_network = QNetwork(state_dim, action_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())  # same initial weights\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "epsilon = EPS_START\n",
    "all_rewards = []\n",
    "\n",
    "# Pre-fill replay buffer with random actions\n",
    "obs, info = env.reset()  # new API: reset returns (obs, info)\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "    obs = next_obs\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(MAX_EPISODES):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action(obs, q_network, epsilon, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward += reward\n",
    "        replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # Sample from replay buffer and update network\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        loss = compute_td_loss(batch, q_network, target_network, optimizer)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "\n",
    "    # Update target network periodically\n",
    "    if (episode + 1) % TARGET_UPDATE_FREQ == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    all_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode+1}, Reward: {episode_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    # Early stopping if environment is solved\n",
    "    # CartPole-v1 is considered solved at average reward >= 195 over 100 consecutive episodes\n",
    "    if len(all_rewards) >= 100 and np.mean(all_rewards[-100:]) >= 195:\n",
    "        print(f\"Solved in {episode+1} episodes!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072349c-692f-4a2c-919f-ac28efa425ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
