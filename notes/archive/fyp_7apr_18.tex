
\documentclass[12pt,a4paper]{report}

% Geometry & layout
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2cm, headheight=15pt]{geometry}

% Fonts and math
\usepackage{amsmath, amssymb, bbm}

% Figures and subfigures
\usepackage{graphicx}
\usepackage{subcaption}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

% Code listings
\usepackage{listings}

% Bibliography
\usepackage{natbib}

% Hyperlinks
\usepackage{hyperref}

% Paragraph formatting
\setlength{\parindent}{0pt}
\usepackage{parskip}
\setlength{\parskip}{\baselineskip}

% Fancy headers/footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{} % clear default header/footer
\fancyhead[L]{\leftmark} % section or chapter title
\fancyhead[R]{\thepage}  % page number
\renewcommand{\headrulewidth}{0.4pt} % optional: adds a horizontal line in the header

% Table of contents depth
\setcounter{tocdepth}{2}
\DeclareMathOperator{\EX}{\mathbb{E}}
% Listings (for code)
\lstset{
  basicstyle=\ttfamily\footnotesize,
  frame=single,
  breaklines=true
}

% Header
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{\leftmark}

% Title Page
\begin{document}
\begin{titlepage}
    \centering
    \vspace*{3cm}
    {\Huge\bfseries Possibility Theory for Reinforcement Learning \par}
    \vspace{2cm}
    {\Large Tejas Gupta \par}
    \vspace{1.5cm}
    Submitted as part of the honours requirements \par
    \vspace{1cm}
    Supervisor: Dr. Jeremie Houssineau \par
    \vfill
    Division of Mathematical Sciences \\
    School of Physical and Mathematical Sciences \\
    Nanyang Technological University \\
    \vspace{1cm}
    \textbf{April 2025}
\end{titlepage}

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
A concise summary of why possibility theory is valuable in reinforcement learning, outlining the three major approaches and their key outcomes.

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
(Optional) Thank your supervisor, friends, colleagues.

\tableofcontents
% \listoffigures
% \listoftables
% \lstlistoflistings

\chapter{Introduction}
\section{Motivation and Context}
\section{Overview of Proposed Methods}
\section{Main Contributions}

\chapter{Background}
\section{Possibility Theory}

Possibility theory, introduced in \cite{ZADEH19999}, is a counterpart to probability theory, that provides an alternative flexible method of messuring and accounting for uncertainty.
In this framework the uncertainty of an event is quantified by a possibility measure; this offers an alternative measure to model uncertainty of an event due to incomplete knowledge.
The possibility of an event can range from $0$ to $1$ where a $0$ possibility would imply that that event is completely impossible where as $1$ would imply that the event is fully possible.
In other words, possibility just refers to the degree with which an event is possible given our current knowledge.
This is in contrast to the probability measure, where probability measures typically refer to the statistically frequency of an event, so probability of $1$ for an event would imply that said event is statistically likely to happen all the time.
Similarly, a possibility of $0.
8$ would imply that the event has high plausibility, where as a probability of $0.8$ would (typically) imply the stronger claim that the event happens with frequency of 0.8. \par

Possibility theory was introduced as an extension to fuzzy sets in \cite{ZADEH19999}. A fuzzy set $\tilde{A}$ is defined as a set of ordered pairs:
\[
  \tilde{A} = \{(x, \mu_{\tilde{A}(x)} \mid x \in X)\} 
\]
where $\mu_{\tilde{A}}: X \to [0,1]$ is the membership function to the fuzzy set. The membership function over the set can also be understood as a \emph{Possibility Distribution $\hat{\pi}(x)$} over the set $X$. Analagous to probability theory, where the sum of the probabilities of all outcome states must be 1, a possibility distribution must ensure that at least one state is fully possible., i.e:
\[
  \sup{x \in X} \hat{\pi}(x) = 1
\]
The induced possibility measure for any subset of states is given by $\hat{\pi}$ is defined as the maximal value of $\hat{\pi}$ over the states. 
\[ \hat{\Pi}(A) = \sup {\hat{\pi}(x) \mid x \in A} \]
The above further implies that the union of 2 disjoint events are maxative. 
\[
  \hat{\Pi}(A \cup B) = \max\{\hat{\Pi}(A),\hat{\Pi}(B)\}.
\]
Note the above still holds if A and B are not disjoint. The possibility measure contrasts sharply with probability measures where the probability of the union of disjoint events is the sum of the probabilities. Similar to probability messures 
\[
\hat{\Pi}(\Omega) = 1
\hat{\Pi}(\varnothing) = 1
\]
\cite{Dubois2001} also introduced the notion of necessity, the dual of possibility of an event. Necessity refers to the lack of plausibility of not an event. In order words an event is necessary to the extend that its complement is impossible. Possibility and Necessity together can be interpreted as upper and lower probability bounds of impressise probabilities (\cite{DUBOIS199265}). 
\[
  N(A) = min(\{1-\hat{\pi}(x)\} \mid x \in A) = 1 - \hat{\Pi}(\neg A)
\]
\subsection{Algebraic Properties: Possibility vs. Probability}

We now describe the main algebraic differences between probability and possibility theories.

\subsubsection{Additivity and Maxitivity}

  Probability measures are \emph{additive}. For any two disjoint events \(A\) and \(B\) (i.e., \(A\cap B=\emptyset\)), the probability of their union is given by
  \[
  P(A \cup B) = P(A) + P(B).
  \]
  This additive property reflects the quantitative nature of probability, where the total weight is distributed among all outcomes.

  In contrast, possibility measures are \emph{maxitive} (or supremum-preserving) (\cite{Dubois:2007}). For any events \(A\) and \(B\), the possibility measure of their union is given by
  \[
    \hat{\Pi}(A \cup B) = \max\{\hat{\Pi}(A), \hat{\Pi}(B)\}.
  \]
  This property means that if at least one event is highly possible, then their union is considered highly possible. This is a key feature that allows possibility theory to express complete ignorance by simply assigning a possibility of 1 to all outcomes without forcing a partition of numerical weights 
 % \cite{en.wikipedia.org, arxiv.org}.

\subsubsection{Normalization}
  A probability distribution over an outcome space \(X\) requires that the probabilities of all states sum to 1:
  \[
  \sum_{x \in X} P(x) = 1.
  \]
  Even in situations of complete ignorance, a uniform distribution is imposed, which still assigns a fractional probability.

  A possibility distribution, on the other hand, is normalized by requiring that at least one outcome has the maximal possibility:
  \[
    \sup_{x \in X} \hat{\pi}(x) = 1.
  \]
  This normalization permits complete ignorance to be represented trivially by assigning \( \hat{\pi}(x)=1 \) for every \(x\) in \(X\). Under such a distribution, each event has a necessity of 0, since
  \[
  N(A) = 1 - \hat{\pi}(A^c) = 0,
  \]
  when nothing is ruled out. This flexibility makes it easier to represent uncertainty qualitatively, without imposing precise quantitative values 

\subsubsection{Intersections and Unions: Conjunctions and Disjunctions}
  
  For independent events \(A\) and \(B\), the probability of the joint event (the intersection) is typically given by the product:
  \[
  P(A \cap B) = P(A) \cdot P(B).
  \]
  Similarly, as stated above, disjoint events have probabilities that add:
  \[
  P(A \cup B) = P(A) + P(B).
  \]
  In contract, possibility theory uses the concept of triangular norms (t-norms) to model the logical AND (conjunction) of events (\cite{DUBOIS01021982}). A common t-norm is the minimum operator, so that for events \(A\) and \(B\) with possibility distributions \( \pi_A(x) \) and \( \pi_B(x) \) respectively, the possibility distribution for the intersection is given by:
  \[
  \pi_{A \cap B}(x) = \min\{ \pi_A(x), \pi_B(x) \}.
  \]
  This indicates that the possibility of a state satisfying both \(A\) and \(B\) is determined by the less possible (the minimum) of the two. Dually, t-conorms (such as the maximum operator) are used for logical OR (disjunction):
  \[
  \pi_{A \cup B}(x) = \max\{ \pi_A(x), \pi_B(x) \}.
  \]
  Thus, while probability theory uses multiplication (for independent events) and addition (for disjoint events), possibility theory replaces these operations with min and max, respectively. This leads to a very different arithmetic of uncertainty, which can make handling incomplete information easier.

\subsubsection{Fuzzy Measures and Integrals}
Possibility measures are a subset of fuzzy messures; fuzzy measure generalizes the classical measure by disregarding additivity and requiring only monotonicity:
\[
A \subseteq B \implies m(A) \leq m(B).
\]
In decision theory, instead of using the expected value (computed via the Lebesgue integral), one can aggregate outcomes using the Sugeno integral, a nonlinear operator based on the max and min operations. The Sugeno integral provides an analogue to the Lebesgue integral and is particularly useful in qualitative decision-making scenarios where the precise numeric integration of probabilities is not possible or desired (\cite{Dubois:2015})
  
For example, a decision-maker employing possibility theory might adopt a maximin strategy: they evaluate each action by the minimum degree of satisfaction (or necessity) it guarantees, and then select the action with the highest such minimum value. Conversely, a maximax strategy would rely on the maximum possible outcome (reflecting an optimistic bias).  

TODO add explanation for this. 

\subsubsection{Example}
TODO 
Lets consider the case of rolling a dice. Let event $a$ and $b$ be the event where the dice yields $4$ and $5$ respectively. Without any information about the result of the dice throw, the probability are $1/6$ each where as the possibility of the events is $1$. If one is informed 
\section{Reinforcement Learning}

Reinforcement Learning is a machine learning framework for agent sequential descision making in an environment. At each timestep the agent observes the state in which it currently is, takes an action which moves it to another state and collects reward (the reward collected can be zero). The notion of Actions, States, Rewards and the associated stochastic transitions is formally known as the Markov Decision Porcess (MDP). Here we will discuss some core Reinforcement Learning concepts along with previous work employing possibility theory. 

\subsection{Markov Decision Provess}

A MDP is defined by the mathematical tuple $(S, A, P, R, \gamma)$ where 
\begin{itemize}
  \item \textbf{State Space $S$:} refers to all possible states in the an environment.  
  \item \textbf{Action Space $A_s$:}  refers to all possible actions available to the agent in the state $s$. In some formulations, the action space $A$ might be same across states. 
  \item \textbf{Transition Probabilities $P(s' \mid s, a)$:}  refers to the probability to transition to state $s'$ by taking the action $a$ in state in $s$. These transtions can be be either stochastic or determinestic.   
  \item \textbf{Reward function $R(s, a, s')$:} is the immediate reward recieved by taking the action $a$ in state $s$ and transtioning to state $s'$. $R(s, a)$ refers to the expected reward recieved by taking action $a$ in state $s$.  
  \item \textbf{Discount Factor $gamma$:} is the discounting factor of future rewards to determine the the current value of the current state. A reward of $1$ obtained after K steps is worth $\gamma ^ K$ at the current step. Trivially, if $\gamma$ is 1 then there is no discounting of future rewards.    
\end{itemize}
 
As the name suggests, the Markov descision process also satisfies the Markov Propery, i.e, the next state $s'$ and the reward $r$ only depends on the current state action pair $(s, a)$; all prior history is irrelevant. The agents behaviour in a state is charectarised by its policty $\pi$ where $\pi(a \mid s)$ refers to probability of the agent enacting $a$ at state $s$. The goal of reinforcement learning to find an optimal policy $\pi^*$ that maximises cummliative rewards in an MDP.  

$R_{t}$ refers to the random variable denoting the reward the agent rewards at timestep $t$. We can further define the cummliative rewards from the time step $t$ as 
\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]
Here both the Reward random variable and the cummliative reward random variable depend on the state at current time $t$ and the policy of the agent $\pi$. The exepected cummliative reward under a given policty is represented by the state-value function $V^{\pi}(s)$. 
\[
  V^{\pi}(s) = \EX_{\pi}[G_t \mid S_t = s]
\]
Similarly, a action value function (Q-value) $Q^\pi (s, a)$ can be defined as the expected cummliative return from state $s$ if the agent takes action $a$. 
 \[
  Q^{\pi}(s) = \EX_{\pi}[G_t \mid S_t = s, A_t = a]
\]

These expectations quantify how good an action or state is in terms of its expected cummliative rewards. Correspondingly, two policies can be compared on a given state by comparing the value functions induced by that policy in that state. An optimal policty, hence, is the policy $\pi^*$ that induces the optimal value function $V^*(s) = \max_\pi V^\pi(s)$ and $Q^*(s, a) = \max_\pi Q^\pi(s, a)$ for all $s, a$. \par

These expected values are also dependent on each other. 
\[
  V^\pi(s) = \EX_{\pi}[Q^\pi(s, A) \mid S = s]
  Q^\pi(s, a) = \EX^\pi[V^\pi(S') \mid S = s, A = a ]
\] 

By substituing the values further, one can construct a recusrive relationship; this is also known as the Bellman Equation.  
\[
  V^\pi(s) = \EX_{\pi}[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_{t}= s]
\]
The state value of the current state is just the same of the transition reward and the discounted state value of the next state. A similar relationshpip exists for the action value function as follows  
\[
  Q^\pi(s, a) = \EX_{\pi}[ R_{t+1} + \gamma Q^\pi(S'_{t+1}, A_{t+1}) \mid S_{t}= s]
\]
The defination of the recusrive expectations can be fully expanded as follows :
\[V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \sum_{s' \in S} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right] \] 
\[
Q^\pi(s,a) = \sum_{s'}P(s'|s,a)\big[ R(s,a,s') + \gamma \sum_{a'}\pi(a'\mid s'),Q^\pi(s',a')\big]
\]
For a given policy, the Bellman Equations are linear. However, for an optimal policy, we have non linear maximisation operations as follows: 
\[
  V^*(s) = max_{a \in A}\EX[ R_{t+1} + \gamma V^*(S_{t+1}) \mid S_{t}= s]
\]

This gives intuivitive result that the optimal value of a state is same as the expected value of taking the best action from the state. The same result also applies toe the Q-function:
\[
Q^*(s, a) = \EX [ R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') \mid S_t = s, A_t = a ]
\]

In a finite state and action space, it is possible to solve the Bellman Optimality Equations to get th eopimal values using value iteration or policty iterations, making it possible to calculate $V^*$ from which it is trivial to deduce an optimal policy. However, in larger and continous environments, this is no longer feasible. Thus reinforcement learning algorithm attempt to either learn the value functins directly or learn a maximising policty by expereincing the MDPs. 


\subsection{Deep-Q-Learning (DQN)}

Q-Learning is a category of reinforcement learning algorithms that focus on on learning the optimal $Q^*$ value for each state action pair by iterative updates. In its simple tabular form, the iteration happens as follows:
\[Q(s, a) \leftarrow Q(s, a) + \alpha [ r + \gamma \max_{a'} Q(s', a') - Q(s, a) ]\]
Here $alpha$ is the learning rate and $(s,a,r,s')$ is one of the expericnced transitions. This update is based on the Bellman Update discussed before. \cite{Watkins1992} showed that this tabular Q-Learning converges to the optimal value if all states are visited infinitely during the learning process and the learning rates satisfy: 

\[
\sum_{t=0}^{\infty} \alpha_t(s, a) = \infty
\]
\[
\sum_{t=0}^{\infty} \alpha_t^2(s, a) < \infty
\]

As mentioned before, the tabular approach to Q-Learning dis not feasible for continuous or large environments. Deep Q Learning provides an alternative method to functionally approximate the Q-Values using deep neural networks. The function $Q(s, a \mid \theta)$ is parametrised by weights $\theta$. In general, it is possibel to to approximate the Q-Values using other methods (such as Linear Functions).
The method for doing Depp Q Learning was first introduced \cite{Mnih2015} where the authors achieved human level performance in various ATARI games. In their paper, the network takes the state of the game (in the form of a raw image) and outputs the approximate Q-Values for each of the set of discrete actions. Common to other Deep Learning Method, Stochastic Gradient Descent is used to update the Q networks, with the loss function derived from the temporal difference (TD) error. Particularly, the Bellman Backup is used as the target for training, i.e the update target $y$ for the transition $(s, a, r, s')$ is: 

\[
  y = r + \gamma max_{a'} Q(s', a'; \Theta^-)
\]
where $\theta^-$ is the parameters of the target network. A taget network is typically a lagged copy of the main Q-Netowrk and helps stabalise the learning process. The target network can either be continously updated using Polyak Averaging $\theta^- \leftarrow \theta^-+ \alpha (\theta - \theta^-)$ or it can be a copied from the main netowrk periodically during the learning process. 
The loss function can be constructed to minimise the mean squared error between $y$ and its own outputs: 
\[
  L(\theta) = \EX[(y - Q(s, a, ; \theta))^2]
\] 
The loss is calculated over a batch $D$ which is a set of tuples $(s, a, r, s')$ expericnced by the agent. Taking the gradient of the loss:
\[
\nabla_{\theta} L(\theta) = \EX_{(s, a, r, s') \sim \mathcal{D}} [ 2 \cdot \left( Q(s, a; \theta) - y \right) \cdot \nabla_{\theta} Q(s, a; \theta) ]
\]
The network is updated by moving the parameters $\theta$ to minimise the loss:
\[
  \theta \leftarrow \theta - \eta \cdot \nabla_{\theta} L(\theta)
\] 
In practice, the networks are not trained using sequential experiences as this can lead to divergence and instability in the learning process. Instead, \cite{Mnih2015} introduced experiential learning. The transitions $(s, a, r, s')$ experienced during training are stored in a replay buffer. During the training step, a mini-batch is randomly sampled from the replay buffer. The random sampling breaks temporal correlations in the data and generally results in smoother learning. Reusing past transitions also improves data efficiency as each transition can be used multiple times to improve the learning process. Q-Learning is a type of off-policy learning, as the method does not directly learn the policy, rather it only learns an estimate of optimal Q-Value $Q^*$; this means that the Q-Values can be trained from any observed transitions in a given environment. 

\subsection{Actor-Critic Methods}
Actor Critic Methods are a type of On-Policy reinforcement learning algorithm, in that they explicitly learn the policy of the actor along with a critic (usually the value functions). In general, the actor decides what actions to take based on the state, while the critic estimates the advantage of the actions. The actor is updated to maximise the advantage estimate of the critic. Actor-Critic methods can handle continuous and larger action spaces better then Q-Learning, while also learning the value estimates to perform stable policy gradient updates. \par 
 
In policy gradient methods, the policy $\pi_\theta(s)$ is parametrised by $\theta$, this is often a implemented using neural networks. An objective function $J(\theta)$ is defined as the expected return from the environment following policy parametrised by $\theta$ from some starting distribution of states. The policy gradient theorem provides a way to express the graient of the objective function $J(\theta)$ as 
\[
\nabla_{\theta} J(\theta) = \EX_{s \sim d^{\pi}, a \sim \pi_{\theta}} \left[ Q^{\pi}(s, a) \nabla_{\theta} \log \pi_{\theta}(a \mid s) \right]
\]
where $s$ is distributed by $d^\pi$ (the visitation frequency of the states under the policy $\pi_\theta$) and $a$ is distrbuted by the policy $\pi_\theta$. This means that the gradient of expected reward can be calculated as the sum of action values weighted by 
\[
\frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} 
\]
The parameter can then updated using 
\[
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
\] \par
In practice, one uses the advantage function $A(s, a) = Q(s, a) - V(s)$ instead of the $Q(s, a)$ value directly as that increases stability in learning. It is shown that replacing the Q-Value with the advantage function does not introduce a bias. 

The actor network is then updated with:
\[
\Delta \theta_{\text{actor}} \propto \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \, \hat{A}_t
\]
where $\hat{a}$ is the empirical advantage.\par 

The critic in actor critic environments is commonly the optimal state-value function approximate $V_w(s)$ or the optimal action-value approximation $Q_w(s,a)$. The critic can be trained on similar to Q-Learning using temporal difference (TD) learning. The state-value critic, for example, can be updated by minimising the error $\delta_t$:
\[
\delta_t = r_{t+1} + \gamma V_{w'}(s_{t+1}) - V_{w}(s_t)
\]
Similar to the DQN learning discussed before, there is usually a target critic and online critic to stabilise learning. Importantly, $ r_{t+1} + \gamma V_{w'}(s_{t+1})$ is an estimator of the Q Value $Q(s,a)$ where the actor $a$ is determined by the current policy $\pi_\theta$; therefore $\delta_t$ itself is an estimator for the advantage function. Hence, the actor's gradient update can be implemented as 
\[\Delta \theta \propto \Delta_\theta log \pi_\theta (a_t \mid s_t)\]

which nudges the policy to increase the probability of action $a_t$ if the observed reward is greater than the expected value estimated by $V_w(s)$. In other words, the critic criticises the actions and the actor uses this to improve its policy. \par

It is also possible to have deterministic policies instead of stochastic policy described above. In the deterministic policy gradient approach (DPG) $a = \mu_\theta$ represnts the determinestic policy produced by the actor. The update to the actors policy is implemented as:
\[
  \nabla_\theta J \approx \EX_{s \sim D}\big[\nabla_a Q_w(s,a)\big|{a=\mu_\theta(s)} ,\nabla_\theta \mu_\theta(s)\big]
\]
Here, the action-value function itself is the critic, and $\mu_\theta$ is updated to maximise the Q-Value. The Q value is trained using the mini-batches, similar to in DQN.  

\subsection{Model-Based Reinforcement Learning}
The algorithms discussed earlier are all model-free, in that, none of the methods maintain an internal "model" of the environment. In contrast, there exists a category of algorithms that use and/or learn a model of the environment. This can include learning the state transitions $\hat{P}(s' \mid s, a)$ and the reward function $\hat{R}(s, a)$. The agent is able to plan with the environment model to evaluate its actions without enacting them in the real environment. \par

In MDP, if the true transition and reward functions are known, it is possible to infer the optimal policies by dynamic programming, this is also referred to as planning. However, in most applications, the exact $P, R$ are unknown and the various Model Based approaches attempt to learn a model of environment through repeated interaction. The learned $P, R$ can then be used for planning in the environment. A major benefit of model based approaches is the improved sample efficiency during training as the agent is able to simulate experience using the environment model instead of taking real actions. Model Based learning can be particularly beneficial if taking samples in the real environment is costly or limited. \par 

In practice, learning the model can be treated as a supervised learning task as it is possible to treat each recorded transition $(s, a, r, s')$ as a training sample for the models $\hat{P}$ and $\hat{R}$. The model can help both in action selection and with value iteration. Dyna-Q Learning is common approach for value iteration, where the Q values are updated by simulated planning steps that are intermixed with real experiences. The model can also help in action selection, for example, it is possible to do Monte Carlo Tree Search to simulate action sequences and select the best performing action.\par

One of the essential challenges of model based learning is learning an accurate model. A biased or imperfect model can create result in non-optimal policies. There are many different approaches to account for this including: shorter planning horizons, uncertainty bounds on the model and ensemble models. Here, we propose one method to address this using possibility bounds on the models. 

\chapter{Proposed Approaches}
\section{Possibility Over Q Values}
\section{Possibility Over Q Ensembles}
\section{Model-Based MaxMax Possibility}

\chapter{Experimental Setup}
\section{Environments}
\section{Implementation Details}

\chapter{Results and Discussion}
\section{Performance Comparison}
\section{Insights}
\section{Limitations}

\chapter{Conclusion}
\section{Summary}
\section{Future Work}

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
\bibliography{fypb.bib}

\appendix
\chapter{Extra Details}
Code, logs, math, anything supplementary.

\end{document}

