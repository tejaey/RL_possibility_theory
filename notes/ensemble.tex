\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\begin{document}

\section*{Possibilistic Ensemble Q-Networks for Epistemic Uncertainty}
\subsection*{Ensemble Q-Networks as Bayesian Approximators of Epistemic Uncertainty}
In model-free deep reinforcement learning (RL), epistemic uncertainty (uncertainty due to limited knowledge) can be captured by maintaining a distribution over Q-value functions. A practical approach to approximate such a dist​
PAPERS.NEURIPS.CC
 to train an ensemble of Q-networks, each representing a plausible hypothesis for the true <span class="math-inline">Q</span>-function【5†L1-L4】. Formally, let <span class="math-inline">Q\(s,a;\\theta\_i\)</span> be the $i$th Q-network with parameters <span class="math-inline">\\theta\_i</span>. The ensemble <span class="math-inline">\\\{Q\_i\\\}\_\{i\=1\}^N</span> can be viewed as sampling from an implicit posterior over <span class="math-inline">Q</span>-functions, providing a Bayesian approximation to the agent’s uncertainty【2†L47-L55】【5†L1-L4】. Prior works show that neural network ensembles yield reliable uncertainty estimates, comparable to explicit Bayesian neural networks【2†L47-L55】. In particular, Bootstrapped Deep Q-Networks (Bootstrapped DQN) use multiple Q-heads to “approximate a distribution over Q-values via the bootstrap”【5†L1-L4】, highlighting that ensembles can effectively represent epistemic uncertainty in value estimates. From a theoretical standpoint, an ensemble of functions <span class="math-inline">\\\{Q\_1, \\dots, Q\_N\\\}</span> can be interpreted as representing a belief distribution over Q-values. Each network <span class="math-inline">Q\_i</span> is trained on the data (e.g. transitions) and will generally differ most in parts of the state space that are poorly explored, where the data is sparse. The disagreement among the ensemble is thus an indicator of epistemic uncertainty: if all <span class="math-inline">Q\_i\(s,a\)</span> predictions agree, the estimate is likely in a well-trained region; if they diverge, it signals high uncertainty【2†L49-L58】. This notion is grounded in Bayesian RL – ideally one would maintain a full posterior <span class="math-inline">P\(Q \\mid \\mathcal\{D\}\)</span>, but using a finite ensemble to approximate it is “simpler while still providing reasonable uncertainty estimates”【2†L49-L58】. Each ensemble member can be seen as one “expert” or hypothesis about the true value function. To quantify belief in each network, we associate a possibility weight <span class="math-inline">\\pi\_i</span> with <span class="math-inline">Q\_i</span>. Unlike a probability, which must sum to 1 across hypotheses, a set of possibility weights <span class="math-inline">\\\{\\pi\_i\\\}</span> forms a possibility distribution: a normalized measure where <span class="math-inline">\\max\_i \\pi\_i \= 1</span>. Intuitively, <span class="math-inline">\\pi\_i</span> reflects how plausible network <span class="math-inline">i</span> is as the true Q-function given the evidence so far. High <span class="math-inline">\\pi\_i</span> means network <span class="math-inline">i</span> is very consistent with the data (thus very possible), while lower <span class="math-inline">\\pi\_j</span> indicates network <span class="math-inline">j</span> appears less plausible. By using possibility theory, we do not force a strictly additive belief allocation, which is advantageous under deep uncertainty. In an uninformed initial state, we might set <span class="math-inline">\\pi\_i^\{\(0\)\}\=1</span> for all <span class="math-inline">i</span> (all models are fully possible), faithfully representing complete ignorance. As learning progresses and data accumulates, these weights are refined based on how well each <span class="math-inline">Q\_i</span> predicts the observed outcomes.

\section*{Possibility-Theoretic Weight Updates for the Ensemble}
We propose three principled update schemes for the possibility weights <span class="math-inline">\\\{\\pi\_i\\\}</span> of the ensemble. Each scheme uses a different metric of model performance to adjust <span class="math-inline">\\pi\_i</span>, drawing inspiration from exponential weighting of expert​
RESEARCHGATE.NET
dient-based uncertainty measures, and possibility theory’s normalization by maxima. Let <span class="math-inline">L\_i\(t\)</span> denote the loss of network <span class="math-inline">Q\_i</span> on newly observed data at time <span class="math-inline">t</span> (for example, a temporal-difference loss on a batch of transitions), and let <span class="math-inline">G\_i\(t\) \= \|\\nabla\_\{\\theta\_i\} L\_i\(t\)\|</span> denote the gradient norm of that loss w.r.t. the network parameters (a measure of how strongly the new data is updating <span class="math-inline">Q\_i</span>). The weight update rul​
RESEARCHGATE.NET
ined as follows:

\subsection*{1. Likelihood-Based Exponential Update}
Each network’s weight is updated in proportion to how well it explains the data, measured by the loss <span class="math-inline">L\_i</span>. We define a pseudo-likelihood for model <span class="math-inline">i</span> as <span class="math-inline">\\mathcal\{L\}\_i \= \\exp\(\-\\beta L\_i\)</span>, where <span class="math-inline">\\beta\>0</span> is a scaling factor. The possibility weight is then updated by normalizing these likelihoods across the ensemble. In a probabilistic approach one would use a Softmax (normalization by sum), but in a possibilistic sense we can normalize b​
GTR.UKRI.ORG
 or max. We consider both for completeness:

\textbf{Softmax-normalized (probabilistic interpretation):}
<span class="math-block">\\pi\_i^\{\(\\text\{new\}\)\} \= \\frac\{\\pi\_i^\{\(\\text\{old\}\)\} \\, e^\{\-\\beta L\_i\}\}\{\\sum\_\{j\=1\}^N \\pi\_j^\{\(\\text\{old\}\)\} \\, e^\{\-\\beta L\_j\}\}\.</span>
If all prior weights are equal, this simplifies to <span class="math-inline">\\pi\_i \\propto e^\{\-\\beta L\_i\}</span>, the classic exponentially weighted update【11†L1-L4】. This rule is analogous to Bayesian updating: if <span class="math-inline">L\_i</span> is (proportional to) the negative log-likelihood of the data under model <span class="math-inline">i</span>, then <span class="math-inline">e^\{\-L\_i\}</span> is proportional to the likelihood, and the above is essentially Bayes’ rule for updating the posterior weight of model <span class="math-inline">i</span>. In fact, the Exponentially Weighted Average (EWA) algorithm in online learning uses exactly this form to weight expert <span class="math-inline">i</span> by <span class="math-inline">e^\{\-\\eta \\cdot \\text\{loss\}\_\{i\}\}</span>【11†L1-L4】. Low-loss (high-likelihood) models receive higher <span class="math-inline">\\pi\_i</span>, and the weights are normalized to sum to 1.

\textbf{Max-normalized (possibi​
RESEARCHGATE.NET
erpretation):}
<span class="math-block">\\pi\_i^\{\(\\text\{new\}\)\} \= \\frac\{e^\{\-\\beta L\_i\}\}\{\\max\_j e^\{\-\\beta L\_j\}\} \= e^\{\-\\beta L\_i \+ \\beta \\min\_j L\_j\}\.</span>
Here the most likely model obtains <span class="math-inline">\\pi\=1</span>, and others are scaled relative to it (yielding a valid possibility distribution with <span class="math-inline">\\max\_i \\pi\_i\=1</span>). This normalizatio​
PROCEEDINGS.NEURIPS.CC
maximum is a hallmark of possibility theory【25†L67-L72】: it preserves the absolute plausibility of the best hypothesis rather than diluting it. If one network’s loss is minimal, it becomes the reference point (possibility 1), and any model with a likelihood within, say, 90% of the best will have <span class="math-inline">\\pi\\approx0\.9</span> instead of a much smaller probability. This conservative weighting avoids over-committing to one model when multiple are almost as good – all near-optimal models remain highly possible. Such an update can be seen as a maximum a posteriori (MAP) model selection rule in the limit <span class="math-inline">\\beta \\to \\infty</span>, focusing on the single most plausible <span class="math-inline">Q</span>-function. We will refer to this special case as the Max-Possibility Update, detailed further below.

\textbf{Justification:} Exponential weight updates are theoretically supported both in sequential prediction and Bayesian inference. They minimize regret in the ​
RESEARCHGATE.NET
orithm* for expert advice by rapidly down-weighting experts with high loss【11†L1-L4】. In a Bayesian sense, if each <span class="math-inline">Q\_i</span> has a prior weight <span class="math-inline">\\pi\_i^\{\\text\{\(old\)\}\}</span>, then multiplying by <span class="math-inline">e^\{\-\\beta L\_i\}</span> and renormalizing is exactly Bayes’ rule under a likelihood <span class="math-inline">\\propto e^\{\-\\beta L\_i\}</span>. Thus, the Likelihood-Based update is rigorously grounded as an approximation of a Bayesian posterior over Q-functions【5†L1-L4】, ensuring that ensemble weights concentrate on models that predict new observations well. We will typically use a moderate <span class="math-inline">\\beta</span> to avoid weight collapse; <span class="math-inline">\\beta</span> can be treated as an inverse temperature controlling how “sharp” the weight distribution is (low <span class="math-inline">\\beta</span> yields nearly equal weights, high <span class="math-inline">\\beta</span> approaches a winner-takes-all selection).

\subsection*{2. Gradient-Norm-Based Possibility Update}
While loss alone measures prediction error, the gradient norm <span class="math-inline">G\_i\=\|\\nabla\_\{\\theta\_i\} L\_i\|</span> indicates how surprising or difficult the new data is for model <span class="math-inline">i</span>. A large <span class="math-inline">G\_i</span> means the model’s parameters must significantly change to accommodate the new data, implying that model <span class="math-inline">i</span> was not well-prepared for this sample (a sign of higher uncertainty for that model). Recent studies use gradient norms as an uncertainty measure, noting that “gradient no​
RESEARCHGATE.NET
nts the difficulty of learning a new sample”【11†L1-L4】 and can serve as a proxy for model confidence. We incorporate this by defining a combined score that penalizes both high loss and high gradient norm. One convenient choice is to define an augmented loss:
<span class="math-block">L'\_i \= L\_i \+ \\lambda \\, \\frac\{G\_i\}\{G\_\{\\max\}\} \\, ,</span>
where <span class="math-inline">G\_\{\\max\}</span> is a normalizing constant (e.g. the maximum gradient norm in the ensemble or a running average) and <span class="math-inline">\\lambda</span> controls the trade-off. This <span class="math-inline">L'\_i</span> increases if either the error is large or the model had to significantly adjust itself (large gradient), treating both as signs that model <span class="math-inline">i</span> is less likely to be correct. Then we apply an exponential update on <span class="math-inline">L'\_i</span>:
<span class="math-block">\\pi\_i^\{\(\\text\{new\}\)\} \\propto \\exp\(\-\\beta \\, L'\_i\) \\, ,</span>
with normalization by sum or max as discussed. Essentially, <span class="math-inline">\\pi\_i</span> will be low if mode​
RESEARCHGATE.NET
 fits the new data poorly and had to undergo a big change (indicating that the data revealed a weakness of that model). On the other hand, if <span class="math-inline">Q\_i</span> already anticipated the outcome (small loss, small gradient), then <span class="math-inline">L'\_i</span> is very small and <span class="math-inline">Q\_i</span>’s weight <span class="math-inline">\\pi\_i</span> will increase markedly. \textbf{Justification:} The inclusion of <span class="math-inline">G\_i</span> is inspired by the notion of uncertainty-based adaptive learning rates and recent gradient-based uncertainty quantification【11†L1-L4】. A model that is very certain about its prediction will typically have learned features such that a new sample causes only a small parameter update (small gradient). Conversely, if a new transition causes a large change in parameters, it suggests that model’s prior knowledge was inadequate (sign of epistemic uncertainty for that model). By combining loss and gradient information, we weight models not just by current error but also by structural alertness. This is related to the idea of tracking the Fisher information or Hessian: if a model is in a region of parameter space with steep loss surface (high gradient norm), it might be less reliable. While classical Bayesian updating doesn’t include gradient norms explicitly, our heuristic is supported by empirical findings that high gradient magnitudes correlate with difficult, informative samples【11†L1-L4】. Thus, the Gradient-Norm-Based update can be seen as a refinement of the likelihood-based scheme, giving an additional penalty to models that exhibit signs of surprise at the new data.

\subsection*{3. Max-Possibility (MAP) Update}
This strategy is an extreme case of the exponential update under possibilistic normalization, focusing on the single most promising model. After observing new data, we identify the model <span class="math-inline">k \= \\arg\\min\_i L\_i</span> with the highest
GTR.UKRI.ORG
smallest loss). We then conservatively update the weights by assigning the highest possibility to <span class="math-inline">k</span> and scaling others relative to it:
<span class="math-block">\\pi\_\{j\\neq k\}^\{\\text\{\(new\)\}\} \= \\frac\{\\mathcal\{L\}\_j\}\{\\mathcal\{L\}\_k\} \\;\=\\; \\frac\{e^\{\-L\_j\}\}\{e^\{\-L\_k\}\}\\,\.</span>
Here model <span class="math-inline">k</span> remains at possibility 1 (fully plausible), and every other model <span class="math-inline">j</span> has <span class="math-inline">\\pi\_j < 1</span> depending on its likelihood ratio to the best model. In practice, if model <span class="math-inline">j</span>’s loss is only slightly higher than <span class="math-inline">k</span>’s, it will get <span class="math-inline">\\pi\_j</span> near 1, whereas if it is much worse, <span class="math-inline">\\pi\_j</span> will drop towards 0. This update does \textbf{not renormalize by any sum} – it preserves the scale that the best model had and only relativizes the rest. If the best and second-best models are close in performance, both will end up with <span class="math-inline">\\pi\\approx 1</span> (reflecting that we essentially consider two almost-consistent hypotheses). If one model vastly outperforms others on the new data, it will stay at 1 while others drop to near 0. Over time, this tends to p&#8203;:contentReference[oaicite:21]{index=21}mble toward a state where one model dominates if it consistently performs best. \textbf{Justification:} The Max-Possibility update is directly rooted in \textbf{possibility theory}. It constructs a possibility distribution from the likelihoods by \textbf{normalizing with the supremum} (the maximum) instead of the sum【25†L67-L72】. In Bayesian terms, this resembles always taking the \textbf{maximum a posteriori} model after each observation, rather than averaging. This approach is \textbf{conservative} in that it avoids diluting the belief in the top-performing model; it hedges less among models compared to the softmax update. The rationale is that under severe epistemic uncertainty, one might prefer to maintain any hypothesis that \textit{could} be true (until definitively ruled out), rather than force a probabilistic trade-off. By keeping the best model at weight 1, we ensure the agent always fully considers the currently most likely hypothesis, while still allowing others a fractional possibility. This method connects to the concept of \textbf{upper probabilities} in imprecise probability theory: the possibility <span class="math-inline">\\pi\_i</span> can be seen as an \textit{upper bound} on the probability that <span class="math-inline">Q\_i</span> is the true model. Indeed, any probability distribution <span class="math-inline">P\(i\)</span> over models must satisfy <span class="math-inline">P\(i\)\\le \\pi\_i</span> for all <span class="math-inline">i</span> in order to be consistent with the possibility assignment【25†L67-L72】. Thus, Max-Possibility updating provides a \textbf{robust envelope} of Bayesian model averaging – it errs on the side of keeping options open. Similar max-normalization rules appear in \textbf{evidential reasoning} (Dempster-Shafer theory) when combining evidence with full conflict: one uses plausibility (max-normalized likelihood) instead of normalized probability. In our context, this update could be useful if we suspect the truth might lie in one model and we want to quickly concentrate on it, yet we do not want to completely discard alternatives unless their likelihood drops to zero. We note that as a downside, this approach can become brittle if noise is present, since it may over-focus on a model that got lucky on a small sample; thus it should be applied with caution or in combination with regularizing resets.

\section*{Action Selection under Ensemble Uncertainty}
Given an ensemble of Q-functions and their possibility weights, we consider two mechanisms for choosing the action in state <span class="math-inline">s</span>: (a) using a \textbf{weighted average Q-value}, and (b) a \textbf{majority vote} among the ensemble’s preferred actions. These correspond to different decision-theoretic principles under uncertainty. We provide formal definitions and rationale for each.

\subsection*{Weighted Average Q-Value Selection}
In this scheme, we compute a \textit{possibility-weighted aggregate} of the Q-values for each action and then select the action with highest aggregate. If <span class="math-inline">\\pi\_i</span> is the (normalized) weight of model <span class="math-inline">i</span>, we define the ensemble Q-value for action <span class="math-inline">a</span> as a \textbf{weighted average}:
$$Q_{\text{ens}}(s,a) \;=\; \sum_{i=1}^N w_i \, Q_i(s,a)\,, $$
where <span class="math-inline">\\\{w\_i\\\}</span> are weights derived from <span class="math-inline">\\\{\\pi\_i\\\}</span>. If the <span class="math-inline">\\pi\_i</span> are treated as probabilities (summing to 1), one natural choice is <span class="math-inline">w\_i\=\\pi\_i</span>. If using purely possibilistic weights (max-normalized), one can either normalize them to sum to 1 (to interpret as a rough probability) or use them in a weighted vote sense (discussed below). :contentReference[oaicite:22]{index=22}If <span class="math-inline">\\sum\_i w\_i</span> sums to 1, <span class="math-inline">Q\_\{\\text\{ens\}\}\(s,a\)</span> is essentially the \textbf{expected Q-value} for action <span class="math-inline">a</span> un&#8203;:contentReference[oaicite:23]{index=23}emble’s belief distribution. The action selection is then:
$$a^*(s) \;=\; \arg\max_{a\in \mathcal{A}} \; Q_{\text{ens}}(s,a)\,. $$
\textbf{Theoretical Motivation:} If the ensemble weights represented a true posterior probability <span class="math-inline">P\(\\theta\_i \\mid \\mathcal\{D\}\)</span> over models, then <span class="math-inline">Q\_\{\\text\{ens\}\}\(s,a\)</span> would be the Bayes-optimal estimate of the Q-value (the posterior mean of <span class="math-inline">Q\(s,a\)</span>). Choosing the action that maximizes this \textbf{expected value} is a risk-neutral decision: it maximizes the agent’s expected return \textit{on average} over model uncertainty. This is analogous to choosing an action by integrating over a distribution of models (also related to \textbf{Thompson sampling} expectations). In fully Bayesian RL, the optimal policy w.r.t. the posterior is indeed to maximize the expected Q (this yields the highest expected utility). Thus, weighted averaging aligns with the principle of \textbf{Bayesian model averaging} in decision-making. In practice, weighted ensemble policies have been found effective in many ensemble learning contexts. For example, Condorcet’s jury theorem and related results show that aggregating estimators (via averaging or voting) tends to improve accuracy if individual models are even slightly better than random【14†L1-L4】. In RL specifically, averaging ensemble predictions has been used to stabilize value estimates【16†L7-L14】. However, one must be cautious: if one model severely overestimates <span class="math-inline">Q\(s,a\)</span> in an unseen state (due to epistemic uncertainty), a probability
