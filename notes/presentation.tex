\documentclass[11pt,aspectratio=169]{beamer}
\DeclareMathOperator{\EX}{\mathbb{E}}
\usetheme{Copenhagen}
%------------------------------------------------
% Packages
%------------------------------------------------
\usepackage{booktabs}                  % professional tables
\usepackage{hyperref}                  % hyperlinks
\usepackage{tikz}                      % diagrams
\usetikzlibrary{shapes.geometric,arrows.meta}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
%------------------------------------------------
% Beamer adjustments
%------------------------------------------------
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{headline}{}
\setbeamercovered{transparent}
%------------------------------------------------
% Title page details
%------------------------------------------------
\title[FYP Thesis]{\large Possibility Theory in Reinforcement Learning}
\subtitle{Final Year Project Presentation}
\author[Tejas Gupta]{\textbf{Tejas Gupta} Supervisor: Dr. Jeremie Houssineau}
\institute[Nanyang Tech. U.]{School of Physical \& Mathematical Sciences\\NTU Singapore}
\date[April 2025]{April 18, 2025}

\begin{document}

\tikzstyle{diam} = [diamond, aspect=2, draw, fill=red!40, text width=6em,text centered ]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=3cm,text centered, rounded corners, minimum height=2em ]
\tikzstyle{trap} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum height=2em, text centered, draw=red, fill=green!30]
\tikzstyle{rect} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=red, fill=orange!30]
\tikzstyle{line} = [draw, -latex]\tikzstyle{highlight} = [draw=orange, fill=yellow!50, thick]
%------------------------------------------------
% Title
%------------------------------------------------
\begin{frame}
  \maketitle
\end{frame}

%------------------------------------------------
% Outline
%------------------------------------------------
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------
\section{Introduction}
%------------------------------------------------
\begin{frame}{Motivation and Problem Statement}
  \begin{itemize}
    \item Why modelling epistemic uncertainty matters in RL.
    \item Can we use possibility theory to model uncertainty in RL?
  \end{itemize}
\end{frame}

\begin{frame}{Outline}
  \begin{itemize}
    \item Introduction to Deep Q Learning
    \item Introduction of Possibility Theory
    \item Possibility over Q Values
    \item Possibility over Q Ensembles
    \item Introduction to Model-Based RL
    \item MaxMax Q Learning
    \item Conclusion
  \end{itemize}
\end{frame}

%------------------------------------------------
\section{Introduction to Q Learning}
%------------------------------------------------
\section{Deep Q-Network (DQN)}

\begin{frame}{DQN Flowchart}
  \begin{columns}

    \begin{column}{0.5\textwidth}
  \only<1>{
    \textbf{Environments} in RL are Markov Decision Processes (MDPs): $(S, A, P, R, \gamma)$
    \begin{itemize}
      \item $S$: Set of all possible states
      \item $A_s$: Actions available in state $s$
      \item $P(s' \mid s, a)$: Transition probability
      \item $R(s, a, s')$: Reward for $(s, a, s')$
      \item $\gamma$: Discount factor
    \end{itemize}
  }
      \only<2>{
        \textbf{Agent} observes current state $s$ and takes action $a$.
        The goal of the agent is to maximise cummalitive rewards:
        \[G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\] 
        Agent behaves according to policy $\pi(a \mid s)$. 

      }
      \only<3>{
        The agent learns \textbf{Q-Values} defined as:
        \[Q^{\pi}(s) = \EX_{\pi}[G_t \mid S_t = s, A_t = a]\]
        \[
          Q^\pi(s, a) = \EX_{\pi}[ R_{t+1} + \gamma Q^\pi(S'_{t+1}, A_{t+1}) \mid S_{t}= s]
        \] 
        This known as Bellman Equation - used to calcuate Q-Values for a policy $\pi$ 
      }
      \only<4>{
        The optimal action given by $\underset{a}{argmax} Q(s,a)$.
        This gives \textbf{Bellman Optimality Equation:}
\[
Q^*(s, a) = \EX [ R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') \mid S_t = s]
\]
      }
    \end{column}

    % Right column: flowchart
    \begin{column}{0.5\textwidth}
      \centering
      \vspace*{1cm}
      \scalebox{1}{%
        \begin{tikzpicture}[node distance=1.5cm and 3.5cm, auto]
          \node[rect]        (Agent) {Agent};
          \node[rect, below=of Agent] (Env)   {Environment};
          % \path[->] (Agent) edge node[right] {$a_t$} (Env)
          %           (Env)   edge[bend right] node[left] {$s_{t+1},r_t$} (Agent);
        \end{tikzpicture}
      }
    \end{column}

  \end{columns}
\end{frame}
\end{document}
