
\documentclass[12pt,a4paper]{report}

% Geometry & layout
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2cm, headheight=15pt]{geometry}

% Fonts and math
\usepackage{amsmath, amssymb, bbm}

% Figures and subfigures
\usepackage{graphicx}
\usepackage{subcaption}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

% Code listings
\usepackage{listings}

% Bibliography
\usepackage{natbib}

% Hyperlinks
\usepackage{hyperref}

% Paragraph formatting
\setlength{\parindent}{0pt}
\usepackage{parskip}
\setlength{\parskip}{\baselineskip}

% Fancy headers/footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{} % clear default header/footer
\fancyhead[L]{\leftmark} % section or chapter title
\fancyhead[R]{\thepage}  % page number
\renewcommand{\headrulewidth}{0.4pt} % optional: adds a horizontal line in the header

% Table of contents depth
\setcounter{tocdepth}{2}
\DeclareMathOperator{\EX}{\mathbb{E}}
% Listings (for code)
\lstset{
  basicstyle=\ttfamily\footnotesize,
  frame=single,
  breaklines=true
}

% Header
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{\leftmark}

% Title Page
\begin{document}
\begin{titlepage}
    \centering
    \vspace*{3cm}
    {\Huge\bfseries Possibility Theory for Reinforcement Learning \par}
    \vspace{2cm}
    {\Large Tejas Gupta \par}
    \vspace{1.5cm}
    Submitted as part of the honours requirements \par
    \vspace{1cm}
    Supervisor: Dr. Jeremie Houssineau \par
    \vfill
    Division of Mathematical Sciences \\
    School of Physical and Mathematical Sciences \\
    Nanyang Technological University \\
    \vspace{1cm}
    \textbf{April 2025}
\end{titlepage}

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
A concise summary of why possibility theory is valuable in reinforcement learning, outlining the three major approaches and their key outcomes.

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
(Optional) Thank your supervisor, friends, colleagues.

\tableofcontents
% \listoffigures
% \listoftables
% \lstlistoflistings

\chapter{Introduction}
\section{Motivation and Context}
\section{Overview of Proposed Methods}
\section{Main Contributions}

\chapter{Background}
\section{Possibility Theory}

Possibility theory, introduced in \cite{ZADEH19999}, is a counterpart to probability theory that provides an alternative, flexible method of measuring and accounting for uncertainty. In this framework the uncertainty of an event is quantified by a possibility measure, which offers an alternative to model uncertainty due to incomplete knowledge. The possibility of an event can range from $0$ to $1$, where a value of $0$ implies that the event is completely impossible and a value of $1$ implies that the event is fully possible. In other words, possibility refers to the degree with which an event is possible given our current knowledge. This is in contrast to probability measures, where a probability of $1$ implies that an event is statistically certain (or highly frequent), while a probability of $0.8$ typically implies that the event happens with a frequency of 80\%.

\subsection{Fuzzy Sets and Possibility Distributions}
Possibility theory was introduced as an extension to fuzzy sets in \cite{ZADEH19999}. A fuzzy set $\tilde{A}$ is defined as a set of ordered pairs:
\[
  \tilde{A} = \{(x, \mu_{\tilde{A}}(x)) \mid x \in X\},
\]
where $\mu_{\tilde{A}}: X \to [0,1]$ is the membership function to the fuzzy set. The membership function over the set can also be understood as a \emph{Possibility Distribution} $\hat{\pi}(x)$ over the set $X$. Analogous to probability theory, where the sum of the probabilities of all outcome states must be 1, a possibility distribution must ensure that at least one state is fully possible, i.e.,
\[
  \sup_{x \in X} \hat{\pi}(x) = 1.
\]
The induced possibility measure for any subset $A \subseteq X$ is defined as the maximal value of $\hat{\pi}$ over the states:
\[
  \hat{\Pi}(A) = \sup \{ \hat{\pi}(x) \mid x \in A \}.
\]
This further implies that the union of two events is maxitive:
\[
  \hat{\Pi}(A \cup B) = \max\{\hat{\Pi}(A),\hat{\Pi}(B)\}.
\]
Note that this holds even if $A$ and $B$ are not disjoint. In contrast to probability measures, where the probability of the union of disjoint events is the sum of the probabilities, possibility measures satisfy
\[
\hat{\Pi}(\Omega) = 1,\quad \hat{\Pi}(\varnothing) = 1.
\]
\cite{Dubois2001} also introduced the notion of necessity, the dual of possibility, defined as
\[
  N(A) = \min\{1-\hat{\pi}(x) \mid x \in A\} = 1 - \hat{\Pi}(\neg A).
\]
Necessity quantifies the lack of plausibility of the complement of an event, so that possibility and necessity together can be interpreted as upper and lower probability bounds of imprecise probabilities (\cite{DUBOIS199265}).

\subsection{Additivity and Maxitivity}
Probability measures are \emph{additive}. For any two disjoint events \(A\) and \(B\) (i.e., \(A\cap B=\emptyset\)), the probability of their union is given by
\[
P(A \cup B) = P(A) + P(B).
\]
This additive property reflects the quantitative nature of probability, where the total weight is distributed among all outcomes.

In contrast, possibility measures are \emph{maxitive} (or supremum-preserving) (\cite{Dubois:2007}). For any events \(A\) and \(B\), the possibility measure of their union is given by
\[
  \hat{\Pi}(A \cup B) = \max\{\hat{\Pi}(A), \hat{\Pi}(B)\}.
\]
This property implies that if at least one event is highly possible, then their union is considered highly possible. It allows possibility theory to express complete ignorance by simply assigning a possibility of 1 to all outcomes without forcing a partition of numerical weights.

\subsection{Normalization}
A probability distribution over an outcome space \(X\) requires that the probabilities of all states sum to 1:
\[
\sum_{x \in X} P(x) = 1.
\]
Even in situations of complete ignorance, a uniform distribution is imposed, which still assigns fractional probabilities to each outcome.

A possibility distribution, on the other hand, is normalized by requiring that at least one outcome has the maximal possibility:
\[
  \sup_{x \in X} \hat{\pi}(x) = 1.
\]
This normalization permits complete ignorance to be represented trivially by assigning \( \hat{\pi}(x)=1 \) for every \(x\) in \(X\). Under such a distribution, each event has a necessity of 0, since
\[
N(A) = 1 - \hat{\pi}(A^c) = 0,
\]
when nothing is ruled out. This flexibility makes it easier to represent uncertainty qualitatively without imposing precise quantitative values.

\subsection{Intersections and Unions: Conjunctions and Disjunctions}
For independent events \(A\) and \(B\), the probability of the joint event (the intersection) is typically given by the product:
\[
P(A \cap B) = P(A) \cdot P(B).
\]
Similarly, disjoint events have probabilities that add:
\[
P(A \cup B) = P(A) + P(B).
\]
In contrast, possibility theory uses triangular norms (t-norms) to model the logical AND (conjunction) of events (\cite{DUBOIS01021982}). A common t-norm is the minimum operator, so that for events \(A\) and \(B\) with possibility distributions \( \pi_A(x) \) and \( \pi_B(x) \) respectively, the possibility distribution for the intersection is given by:
\[
\pi_{A \cap B}(x) = \min\{ \pi_A(x), \pi_B(x) \}.
\]
This indicates that the possibility of a state satisfying both \(A\) and \(B\) is determined by the lesser possibility of the two. Dually, t-conorms (such as the maximum operator) are used for logical OR (disjunction):
\[
\pi_{A \cup B}(x) = \max\{ \pi_A(x), \pi_B(x) \}.
\]
Thus, while probability theory uses multiplication (for independent events) and addition (for disjoint events), possibility theory replaces these operations with the minimum and maximum operators, respectively. This results in a very different arithmetic of uncertainty, which can simplify the handling of incomplete information.

\subsection{Fuzzy Measures and Integrals}
Possibility measures are a subset of fuzzy measures, which generalize classical measures by relaxing the requirement of additivity and requiring only monotonicity:
\[
A \subseteq B \implies m(A) \leq m(B).
\]
In possibility theory, rather than using the expected value computed via the Lebesgue integral, it is possible to aggregate outcomes using the Sugeno integralâ€”a nonlinear operator based on the max and min operations. The Sugeno integral serves as an analogue to the Lebesgue integral and is particularly useful in qualitative decision-making scenarios where precise numeric integration is neither possible nor desired (\cite{Dubois:2015}).

\section{Reinforcement Learning}

Reinforcement Learning is a machine learning framework for an agent's sequential decision making in an environment. At each timestep, the agent observes the state in which it currently is, takes an action which moves it to another state, and collects a reward (the reward collected can be zero). \par

The notion of Actions, States, Rewards, and the associated stochastic transitions is formally known as the Markov Decision Process (MDP). Here we will discuss some core Reinforcement Learning concepts along with previous work employing possibility theory. \par

\subsection{Markov Decision Process}

A MDP is defined by the mathematical tuple $(S, A, P, R, \gamma)$ where \par

\begin{itemize}
  \item \textbf{State Space $S$:} refers to all possible states in an environment.  
  \item \textbf{Action Space $A_s$:} refers to all possible actions available to the agent in the state $s$. In some formulations, the action space $A$ might be the same across states. 
  \item \textbf{Transition Probabilities $P(s' \mid s, a)$:} refers to the probability of transitioning to state $s'$ by taking the action $a$ in state $s$. These transitions can be either stochastic or deterministic.   
  \item \textbf{Reward function $R(s, a, s')$:} is the immediate reward received by taking the action $a$ in state $s$ and transitioning to state $s'$. $R(s, a)$ refers to the expected reward received by taking action $a$ in state $s$.  
  \item \textbf{Discount Factor $\gamma$:} is the discounting factor of future rewards to determine the current value of the current state. A reward of $1$ obtained after $K$ steps is worth $\gamma^K$ at the current step. Trivially, if $\gamma$ is 1 then there is no discounting of future rewards.    
\end{itemize} \par

As the name suggests, the Markov decision process also satisfies the Markov Property, i.e., the next state $s'$ and the reward $r$ only depend on the current state-action pair $(s, a)$; all prior history is irrelevant. \par

The agent's behaviour in a state is characterised by its policy $\pi$, where $\pi(a \mid s)$ refers to the probability of the agent enacting $a$ at state $s$. The goal of reinforcement learning is to find an optimal policy $\pi^*$ that maximises cumulative rewards in an MDP. \par

$R_{t}$ refers to the random variable denoting the reward the agent receives at timestep $t$. We can further define the cumulative rewards from the time step $t$ as 
\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\] \par

Here, both the reward random variable and the cumulative reward random variable depend on the state at the current time $t$ and the policy of the agent $\pi$. The expected cumulative reward under a given policy is represented by the state-value function $V^{\pi}(s)$.
\[
  V^{\pi}(s) = \EX_{\pi}[G_t \mid S_t = s]
\] \par

Similarly, an action value function (Q-value) $Q^\pi (s, a)$ can be defined as the expected cumulative return from state $s$ if the agent takes action $a$. 
\[
  Q^{\pi}(s) = \EX_{\pi}[G_t \mid S_t = s, A_t = a]
\] \par

These expectations quantify how good an action or state is in terms of its expected cumulative rewards. Correspondingly, two policies can be compared on a given state by comparing the value functions induced by that policy in that state. An optimal policy, hence, is the policy $\pi^*$ that induces the optimal value function $V^*(s) = \max_\pi V^\pi(s)$ and $Q^*(s, a) = \max_\pi Q^\pi(s, a)$ for all $s, a$. \par

These expected values are also dependent on each other.
\[
  V^\pi(s) = \EX_{\pi}[Q^\pi(s, A) \mid S = s]
\]
\[
  Q^\pi(s, a) = \EX^\pi[V^\pi(S') \mid S = s, A = a ]
\] \par

By substituting the values further, one can construct a recursive relationship; this is also known as the Bellman Equation.  
\[
  V^\pi(s) = \EX_{\pi}[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_{t}= s]
\] \par

The state value of the current state is just the same as the transition reward and the discounted state value of the next state. A similar relationship exists for the action value function as follows  
\[
  Q^\pi(s, a) = \EX_{\pi}[ R_{t+1} + \gamma Q^\pi(S'_{t+1}, A_{t+1}) \mid S_{t}= s]
\] \par

The definition of the recursive expectations can be fully expanded as follows:
\[
V^{\pi}(s) = \sum_{a \in A} \pi(a \mid s) \sum_{s' \in S} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right]
\]
\[
Q^\pi(s,a) = \sum_{s'}P(s'|s,a)\big[ R(s,a,s') + \gamma \sum_{a'}\pi(a'\mid s'),Q^\pi(s',a')\big]
\] \par

For a given policy, the Bellman Equations are linear. However, for an optimal policy, we have nonlinear maximisation operations as follows: 
\[
  V^*(s) = \max_{a \in A}\EX[ R_{t+1} + \gamma V^*(S_{t+1}) \mid S_{t}= s]
\] \par

This gives the intuitive result that the optimal value of a state is the same as the expected value of taking the best action from the state. The same result also applies to the Q-function:
\[
Q^*(s, a) = \EX [ R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') \mid S_t = s, A_t = a ]
\] \par

In a finite state and action space, it is possible to solve the Bellman Optimality Equations to get the optimal values using value iteration or policy iteration, making it possible to calculate $V^*$ from which it is trivial to deduce an optimal policy. However, in larger and continuous environments, this is no longer feasible. Thus, reinforcement learning algorithms attempt to either learn the value functions directly or learn a maximising policy by experiencing the MDP. \par

\subsection{Deep-Q-Learning (DQN)}

Q-Learning is a category of reinforcement learning algorithms that focus on on learning the optimal $Q^*$ value for each state action pair by iterative updates. In its simple tabular form, the iteration happens as follows:
\[Q(s, a) \leftarrow Q(s, a) + \alpha [ r + \gamma \max_{a'} Q(s', a') - Q(s, a) ]\]
Here $alpha$ is the learning rate and $(s,a,r,s')$ is one of the expericnced transitions. This update is based on the Bellman Update discussed before. \cite{Watkins1992} showed that this tabular Q-Learning converges to the optimal value if all states are visited infinitely during the learning process and the learning rates satisfy: 

\[
\sum_{t=0}^{\infty} \alpha_t(s, a) = \infty
\]
\[
\sum_{t=0}^{\infty} \alpha_t^2(s, a) < \infty
\]

As mentioned before, the tabular approach to Q-Learning dis not feasible for continuous or large environments. Deep Q Learning provides an alternative method to functionally approximate the Q-Values using deep neural networks. The function $Q(s, a \mid \theta)$ is parametrised by weights $\theta$. In general, it is possibel to to approximate the Q-Values using other methods (such as Linear Functions).
The method for doing Depp Q Learning was first introduced \cite{Mnih2015} where the authors achieved human level performance in various ATARI games. In their paper, the network takes the state of the game (in the form of a raw image) and outputs the approximate Q-Values for each of the set of discrete actions. Common to other Deep Learning Method, Stochastic Gradient Descent is used to update the Q networks, with the loss function derived from the temporal difference (TD) error. Particularly, the Bellman Backup is used as the target for training, i.e the update target $y$ for the transition $(s, a, r, s')$ is: 

\[
  y = r + \gamma max_{a'} Q(s', a'; \Theta^-)
\]
where $\theta^-$ is the parameters of the target network. A taget network is typically a lagged copy of the main Q-Netowrk and helps stabalise the learning process. The target network can either be continously updated using Polyak Averaging $\theta^- \leftarrow \theta^-+ \alpha (\theta - \theta^-)$ or it can be a copied from the main netowrk periodically during the learning process. 
The loss function can be constructed to minimise the mean squared error between $y$ and its own outputs: 
\[
  L(\theta) = \EX[(y - Q(s, a, ; \theta))^2]
\] 
The loss is calculated over a batch $D$ which is a set of tuples $(s, a, r, s')$ expericnced by the agent. Taking the gradient of the loss:
\[
\nabla_{\theta} L(\theta) = \EX_{(s, a, r, s') \sim \mathcal{D}} [ 2 \cdot \left( Q(s, a; \theta) - y \right) \cdot \nabla_{\theta} Q(s, a; \theta) ]
\]
The network is updated by moving the parameters $\theta$ to minimise the loss:
\[
  \theta \leftarrow \theta - \eta \cdot \nabla_{\theta} L(\theta)
\] 
In practice, the networks are not trained using sequential experiences as this can lead to divergence and instability in the learning process. Instead, \cite{Mnih2015} introduced experiential learning. The transitions $(s, a, r, s')$ experienced during training are stored in a replay buffer. During the training step, a mini-batch is randomly sampled from the replay buffer. The random sampling breaks temporal correlations in the data and generally results in smoother learning. Reusing past transitions also improves data efficiency as each transition can be used multiple times to improve the learning process. Q-Learning is a type of off-policy learning, as the method does not directly learn the policy, rather it only learns an estimate of optimal Q-Value $Q^*$; this means that the Q-Values can be trained from any observed transitions in a given environment. 

\subsection{Actor-Critic Methods}
Actor Critic Methods are a type of On-Policy reinforcement learning algorithm, in that they explicitly learn the policy of the actor along with a critic (usually the value functions). In general, the actor decides what actions to take based on the state, while the critic estimates the advantage of the actions. The actor is updated to maximise the advantage estimate of the critic. Actor-Critic methods can handle continuous and larger action spaces better then Q-Learning, while also learning the value estimates to perform stable policy gradient updates. \par 
 
In policy gradient methods, the policy $\pi_\theta(s)$ is parametrised by $\theta$, this is often a implemented using neural networks. An objective function $J(\theta)$ is defined as the expected return from the environment following policy parametrised by $\theta$ from some starting distribution of states. The policy gradient theorem provides a way to express the graient of the objective function $J(\theta)$ as 
\[
\nabla_{\theta} J(\theta) = \EX_{s \sim d^{\pi}, a \sim \pi_{\theta}} \left[ Q^{\pi}(s, a) \nabla_{\theta} \log \pi_{\theta}(a \mid s) \right]
\]
where $s$ is distributed by $d^\pi$ (the visitation frequency of the states under the policy $\pi_\theta$) and $a$ is distrbuted by the policy $\pi_\theta$. This means that the gradient of expected reward can be calculated as the sum of action values weighted by 
\[
\frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} 
\]
The parameter can then updated using 
\[
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
\] \par
In practice, one uses the advantage function $A(s, a) = Q(s, a) - V(s)$ instead of the $Q(s, a)$ value directly as that increases stability in learning. It is shown that replacing the Q-Value with the advantage function does not introduce a bias. 

The actor network is then updated with:
\[
\Delta \theta_{\text{actor}} \propto \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \, \hat{A}_t
\]
where $\hat{a}$ is the empirical advantage.\par 

The critic in actor critic environments is commonly the optimal state-value function approximate $V_w(s)$ or the optimal action-value approximation $Q_w(s,a)$. The critic can be trained on similar to Q-Learning using temporal difference (TD) learning. The state-value critic, for example, can be updated by minimising the error $\delta_t$:
\[
\delta_t = r_{t+1} + \gamma V_{w'}(s_{t+1}) - V_{w}(s_t)
\]
Similar to the DQN learning discussed before, there is usually a target critic and online critic to stabilise learning. Importantly, $ r_{t+1} + \gamma V_{w'}(s_{t+1})$ is an estimator of the Q Value $Q(s,a)$ where the actor $a$ is determined by the current policy $\pi_\theta$; therefore $\delta_t$ itself is an estimator for the advantage function. Hence, the actor's gradient update can be implemented as 
\[\Delta \theta \propto \Delta_\theta log \pi_\theta (a_t \mid s_t)\]

which nudges the policy to increase the probability of action $a_t$ if the observed reward is greater than the expected value estimated by $V_w(s)$. In other words, the critic criticises the actions and the actor uses this to improve its policy. \par

It is also possible to have deterministic policies instead of stochastic policy described above. In the deterministic policy gradient approach (DPG) $a = \mu_\theta$ represnts the determinestic policy produced by the actor. The update to the actors policy is implemented as:
\[
  \nabla_\theta J \approx \EX_{s \sim D}\big[\nabla_a Q_w(s,a)\big|{a=\mu_\theta(s)} ,\nabla_\theta \mu_\theta(s)\big]
\]
Here, the action-value function itself is the critic, and $\mu_\theta$ is updated to maximise the Q-Value. The Q value is trained using the mini-batches, similar to in DQN.  

\subsection{Model-Based Reinforcement Learning}
The algorithms discussed earlier are all model-free, in that, none of the methods maintain an internal "model" of the environment. In contrast, there exists a category of algorithms that use and/or learn a model of the environment. This can include learning the state transitions $\hat{P}(s' \mid s, a)$ and the reward function $\hat{R}(s, a)$. The agent is able to plan with the environment model to evaluate its actions without enacting them in the real environment. \par

In MDP, if the true transition and reward functions are known, it is possible to infer the optimal policies by dynamic programming, this is also referred to as planning. However, in most applications, the exact $P, R$ are unknown and the various Model Based approaches attempt to learn a model of environment through repeated interaction. The learned $P, R$ can then be used for planning in the environment. A major benefit of model based approaches is the improved sample efficiency during training as the agent is able to simulate experience using the environment model instead of taking real actions. Model Based learning can be particularly beneficial if taking samples in the real environment is costly or limited. \par 

In practice, learning the model can be treated as a supervised learning task as it is possible to treat each recorded transition $(s, a, r, s')$ as a training sample for the models $\hat{P}$ and $\hat{R}$. The model can help both in action selection and with value iteration. Dyna-Q Learning is common approach for value iteration, where the Q values are updated by simulated planning steps that are intermixed with real experiences. The model can also help in action selection, for example, it is possible to do Monte Carlo Tree Search to simulate action sequences and select the best performing action.\par

One of the essential challenges of model based learning is learning an accurate model. A biased or imperfect model can create result in non-optimal policies. There are many different approaches to account for this including: shorter planning horizons, uncertainty bounds on the model and ensemble models. 

\section{Possibility Theory and Reinforcement Learning}
Reinforcement Learning algorithm primarily deal with uncertainty in two different forms: aleatoric uncertainty and epistemic uncertainty. Aleatoric uncertainty refers to the uncertainty in the environment because of the inherent randomness; this could include randomness in state transitions and stochastic rewards. Epistemic uncertainty, on the other hand, refers to the uncertainty in the environment because of our lack of knowledge/ information. Possibility Theory can be helpful to evaluate and make use of the latter. 

\subsection{Distributional Reinforcement Learning}
One possible method of handling uncertainty in the environment is by maintaining distributions over $Q(s,a)$ or $V(s,a)$ instead of a single value. Along the same line, distributional Reinforcement Learning, introduced in \cite{bellemare2017}, provides a novel method to handle uncertainty in the reward distribution by modelling a distribution $Z^\pi$ over Q Values. In particular, instead of learning only 
\[
  Q^\pi(s,a) = \EX[Z^\pi(s,a)]
\]
where $Z^\pi$ is the random return, satisfying the above expectation and the recursive relationship 
\[Z^\pi(s,a) \overset{D}{=} R(s,a) + \gamma Z^\pi(s', a')\]. 

The above return also satisfies the Bellman Equation:
\[
Z^*(s,a) \overset{D}{=} R_{t+1} + \gamma \, Z^*\Big(S_{t+1}, \, \arg\max_{a'} \mathbb{E}\big[Z^*(S_{t+1}, a')\big]\Big)
\]
\par
The algorithm C51 in \cite{bellemare2017} works by maintaining a probability distribution over 51 possible values $\{z_1, z_2, ..., Z_51\}$, which are placed uniformly over the range of returns. In particular, instead of the Q-Network outputting a single float value of state action pair, the network returns a probability tuple $(P(Z(s,a) = z_1),P(Z(s,a) = z_2), ... , P(Z(s,a) = z_51))$. For training, a target distribution is calculated as: 
\[T(s,a) \overset{D}{=} r + \gamma Z(s', a^*)\]
where $a^*$ is the greedy action that maximises $\EX[Z(s', a)]$. \par

% TODO Address this with our algorithm using ensembles. 
One drawback of the approach is that the two different uncertainties cannot be easily decoupled. If information about the epistemic uncertainty of the distribution was available, it would be possible to incentive the agent to explore states with less certainty. 

\subsection{Possibilistic Q Learning}
 
Possibilistic Q Learning, introduced by Jeremie, developed further approaches to account for both aleatoric and epestemic uncertainity in Q-Learning with the help of possibility theory. Particuarly, Jeremie utilizes the notion of Maximum expected value defined as 
\[
  \bar{\EX} (\Phi(\psi)) = \sup{sup}{\psi \in \Psi} \{\Phi(\psi) f_\psi(\psi)\}
\]where $f_\psi$ is the possibility function over $\psi$. 
Correspondingly, the most credible value of $\omega$ is the value for which the possibility function $f_\psi$ is maximised: 
\[
  \EX^* (\psi) = \underset{argmax}{\psi \in \Psi} f_\psi(\psi)
\]. 
\par 

Using the above defined expected values, the following recursive relationship can be constructed on the maximum expected Q-value:

\[
Q(s,a) = \mathbb{E}_{S'}\Big[\bar{r}(s,a,S') + \max_{a'} \bar{Q}(s', a' \mid S') \Big]
\]. 

Specifically, here the model mantains a possibility function $P(\dot \mid s, a)$ over the state transitions, and the above updates the value using the notion maximum expected value with respect to the state transitions. The possibility of the state transitions is updated during training, along with the maximum expcted value. This set up promotes exploring states particularly where the possibility is high and the inivtial tabular Q learning values are set to be a maximum, this promotes exploration in the algorithm. 







\chapter{Proposed Approaches}
\section{Possibility Over Q Values}


\section{Possibility Over Q Ensembles}
\section{Model-Based MaxMax Possibility}

\chapter{Experimental Setup}
\section{Environments}
\section{Implementation Details}

\chapter{Results and Discussion}
\section{Performance Comparison}
\section{Insights}
\section{Limitations}

\chapter{Conclusion}
\section{Summary}
\section{Future Work}

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
\bibliography{fypb.bib}

\appendix
\chapter{Extra Details}
Code, logs, math, anything supplementary.

\end{document}

