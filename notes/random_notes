Action Selection under Ensemble Uncertainty
Given an ensemble of Q-functions and their possibility weights, we consider two mechanisms for choosing the action in state $s$:
(a) using a weighted average Q-value, and
(b) a majority vote among the ensembleâ€™s preferred actions.

These correspond to different decision-theoretic principles under uncertainty. We provide formal definitions and rationale for each.

Weighted Average Q-Value Selection
In this scheme, we compute a possibility-weighted aggregate of the Q-values for each action and then select the action with highest aggregate. If $\pi_i$ is the (normalized) weight of model $i$, we define the ensemble Q-value for action $a$ as a weighted average:

ğ‘„ ens ( ğ‘  , ğ‘) = âˆ‘ ğ‘– = 1 ğ‘ ğ‘¤ ğ‘– â€‰ ğ‘„ ğ‘– ( ğ‘  , ğ‘) , Q ens â€‹ (s,a)= i=1 âˆ‘ N â€‹ w i â€‹ Q i â€‹ (s,a),
where ${w_i}$ are weights derived from ${\pi_i}$. If the $\pi_i$ are treated as probabilities (summing to 1), one natural choice is $w_i=\pi_i$. If using purely possibilistic weights (max-normalized), one can either normalize them to sum to 1 (to interpret as a rough probability) or use them in a weighted vote sense (discussed below).

Assuming $w_i$ sums to 1, $Q_{\text{ens}}(s,a)$ is essentially the expected Q-value for action $a$ under the ensembleâ€™s belief distribution. The action selection is then:

ğ‘ âˆ— ( ğ‘ ) = arg â¡ max â¡ ğ‘ âˆˆ ğ´ ğ‘„ ens ( ğ‘  , ğ‘) . a âˆ— (s)=arg aâˆˆA max â€‹ Q ens â€‹ (s,a).
Theoretical Motivation:
If the ensemble weights represented a true posterior probability $P(\theta_i \mid \mathcal{D})$ over models, then $Q_{\text{ens}}(s,a)$ would be the Bayes-optimal estimate of the Q-value (the posterior mean of $Q(s,a)$). Choosing the action that maximizes this expected value is a risk-neutral decision: it maximizes the agentâ€™s expected return on average over model uncertainty. This is analogous to choosing an action by integrating over a distribution of models (also related to Thompson sampling expectations).

In fully Bayesian RL, the optimal policy w.r.t. the posterior is indeed to maximize the expected Q (this yields the highest expected utility). Thus, weighted averaging aligns with the principle of Bayesian model averaging in decision-making.

In practice, weighted ensemble policies have been found effective in many ensemble learning contexts. For example, Condorcetâ€™s jury theorem and related results show that aggregating estimators (via averaging or voting) tends to improve accuracy if individual models are even slightly better than random. In RL specifically, averaging ensemble predictions has been used to stabilize value estimates.

However, one must be cautious: if one model severely overestimates $Q(s,a)$ in an unseen state (due to epistemic uncertainty), a probability-weighted average will inflate $Q_{\text{ens}}(s,a)$ in proportion to that modelâ€™s weight. If the weight $\pi_i$ is not too low, the ensemble could optimistically select an action that only a minority of models favor, simply because that one modelâ€™s Q-value is extreme.

This can sometimes be desirable for exploration (picking potentially high-reward actions that most models are unsure about), but it can also lead to instability if outlier predictions are wrong.

Majority Vote on Preferred Actions
An alternative is to base the decision on each modelâ€™s argmax action. Let

ğ‘ ğ‘– âˆ— = arg â¡ max â¡ ğ‘ ğ‘„ ğ‘– ( ğ‘  , ğ‘) a i âˆ— â€‹ =arg a max â€‹ Q i â€‹ (s,a)
be the best action according to model $i$. In a majority vote scheme, the agent considers the set
{ ğ‘ 1 âˆ— , ğ‘ 2 âˆ— , â€¦ , ğ‘ ğ‘ âˆ— } {a 1 âˆ— â€‹ ,a 2 âˆ— â€‹ ,â€¦,a N âˆ— â€‹ }
and selects the action that appears most frequently. In other words, $a^*(s)$ is the action chosen by the majority of ensemble members (with ties broken by secondary criteria, e.g. using weights or randomly). This can be extended to weighted voting by giving each model $i$ a vote proportional to $\pi_i$, but the core idea is a hard decision based on counting preferences rather than averaging values. If the ensemble size $N$ is large, majority vote corresponds to taking the mode of the modelsâ€™ argmax choices.

Theoretical Motivation:
Majority voting is a form of robust decision rule under uncertainty. Instead of relying on possibly skewed Q magnitudes, it trusts the consensus of the ensemble. This can protect against cases where one model (or a small subset of models) has wildly erroneous estimates. Notably, even if one model predicts an extremely high $Q$ for some action, that action will not be chosen unless at least half of the models agree it is the best. This mitigates the effect of outliers.

In fact, experimental evidence suggests that in ensemble RL, a majority vote policy can outperform naive Q-value averaging. Hans & Udluft (2010) combined multiple neural Q-functions and found that â€œmajority voting is superior to Q-averagingâ€ in producing robust policies. The intuition is that majority voting â€œsmooths outâ€ errors differently: an overestimation by one network is essentially clipped by the dissenting votes of others, leading to more conservative action selection.

One can draw parallels to classification ensembles: averaging corresponds to taking mean predicted probabilities (soft voting), whereas majority vote is hard voting. In classification theory, if each model has accuracy above 50%, majority vote improves overall accuracy (Condorcetâ€™s theorem). In Q-learning, the analogy is looser, but if each networkâ€™s policy has a decent chance of being correct, the majority vote will often select a reliable action.

It effectively uses the wisdom of the crowd: an action that many models independently find good is likely truly good, whereas an action hailed only by a lone model but not by others is suspicious. This is particularly relevant under epistemic uncertainty: if an action has never been tried, some models might randomly think itâ€™s great (due to random initialization or lack of data), but most models will be more cautious. Majority vote would lean towards the cautious view until more evidence accumulates, thereby avoiding leaps of faith based on one optimistic guess.

Weighted vote variant:
If models have unequal weights $\pi_i$, a refined approach is to give each model a vote weight equal to $\pi_i$ and then select the action with the highest total weight. This reduces to simple majority when weights are equal. If possibility weights are used (max-normalized), the top model has weight 1 and others less, which can make the voting tantamount to following the current best model unless others collectively outweigh it. Care must be taken in design to ensure this doesnâ€™t just collapse to the single-model policy.

Summary:
The two action selection mechanisms offer a trade-off. Weighted average Q selection is grounded in maximizing expected return and will take calculated risks when a subset of models predict a high payoff. Majority vote selection leans towards risk-aversion with respect to model uncertainty, requiring broad agreement before committing to an action.

In our project, we consider both mechanisms: the former aligns with Bayesian optimality if the weights are trusted probabilities, and the latter aligns with a robust control philosophy, potentially safer in face of epistemic uncertainty. Empirical evaluation would reveal which yields better learning and exploration in environments with sparse data.



Meanâ€“Var Networks for Uncertainty Quantification
In many reinforcement learning applications, it is useful not only to estimate a mean value (e.g., the Q-value) but also to quantify the uncertainty associated with that estimate. A meanâ€“variance network extends the traditional Q-network by simultaneously predicting both the mean 
ğœ‡ ( ğ‘  , ğ‘) Î¼(s,a) and the variance ğœ 2 ( ğ‘  , ğ‘) Ïƒ 2 (s,a) of the Q-values for each stateâ€“action pair ( ğ‘  , ğ‘) (s,a). Here, the variance serves as an indicator of uncertainty (and conversely, lower variance suggests higher confidence).

1. Concept and Motivation
A meanâ€“var network can be seen as an instance of distributional reinforcement learning. However, the focus here is on using the variance as a proxy for epistemic uncertainty:

Mean Prediction 
ğœ‡ ( ğ‘  , ğ‘)
Î¼(s,a): The estimated expected Q-value for taking action 
ğ‘ a in state ğ‘  s.

Variance Prediction 
ğœ 2 ( ğ‘  , ğ‘) Ïƒ 2
 (s,a): A measure of the network's uncertainty about its Q-value estimate.

In our ensemble context, each network 
ğ‘„ ğ‘– Q i â€‹ could be replaced by (or augmented with) a meanâ€“var network that outputs 
ğœ‡ ğ‘– ( ğ‘  , ğ‘) Î¼ i
â€‹
 (s,a) and 
ğœ ğ‘– 2 ( ğ‘  , ğ‘) Ïƒ i 2
â€‹
 (s,a). The variance provides extra information: if a network is very uncertain (i.e. 
ğœ ğ‘– 2 ( ğ‘  , ğ‘) Ïƒ i 2
â€‹
 (s,a) is high), thenâ€”even if its mean is highâ€”it should receive a lower â€œpossibilityâ€ or weight when combining estimates.

The idea is to use the variance as an inverse proxy for certainty. For instance, one might define the possibility weight for network 
ğ‘– i at ( ğ‘  , ğ‘)
(s,a) as:
ğ‘ ğ‘– ( ğ‘  , ğ‘) = exp â¡ ( âˆ’ ğœ† â€‰ ğœ ğ‘– 2 ( ğ‘  , ğ‘)) , p i â€‹ (s,a)=exp(âˆ’Î»Ïƒ i 2 â€‹ (s,a)),
where 
ğœ† > 0
Î»>0 is a tuning parameter. In this formulation, networks with lower variance (more confident predictions) receive higher possibility weights, while those with high uncertainty are downweighted. The exponential function ensures that possibility remains in the interval 
( 0 , 1 ]
(0,1] and that the best (i.e., lowest variance) network achieves a possibility near 1.

2. Implementation Considerations
To integrate a meanâ€“var network into the ensemble, you would make the following modifications to your current framework:

Network Architecture:
Modify the Q-network architecture so that its final layer outputs two quantities for each action: the mean 
ğœ‡ ğ‘– ( ğ‘  , ğ‘) Î¼ i
â€‹
 (s,a) and the variance 
ğœ ğ‘– 2 ( ğ‘  , ğ‘) Ïƒ i 2 â€‹
 (s,a). The variance output should be constrained to be non-negative (e.g., by learning the log-variance and then exponentiating, or by applying a softplus activation function).

Loss Function:
The loss function for each network will consist of two terms:

A mean squared error (MSE) term comparing the predicted mean 
ğœ‡ ğ‘– ( ğ‘  , ğ‘) Î¼ i â€‹
 (s,a) to the target 
ğ‘¦ ğ‘– y i â€‹
  (as computed by your Bellman update).

A term that encourages proper calibration of the variance. One common approach is to use the negative log-likelihood (NLL) for a Gaussian distribution. Assuming a Gaussian model for the Q-value estimate:

ğ‘„ ğ‘– ( ğ‘  , ğ‘)
âˆ¼
ğ‘ ( ğœ‡ ğ‘– ( ğ‘  , ğ‘) , ğœ ğ‘– 2 ( ğ‘  , ğ‘)) , Q i
â€‹
 (s,a)âˆ¼N(Î¼ i â€‹
 (s,a),Ïƒ i 2 â€‹
 (s,a)),
the negative log-likelihood is given by

NLL ğ‘– ( ğ‘  , ğ‘) = ( ğ‘¦ ğ‘– âˆ’ ğœ‡ ğ‘– ( ğ‘  , ğ‘)) 2 2 ğœ ğ‘– 2 ( ğ‘  , ğ‘) + 1 2
log
â¡
ğœ ğ‘– 2 ( ğ‘  , ğ‘) +
constant
.
NLL 
i
â€‹
 (s,a)= 2Ïƒ i 2
â€‹
 (s,a) (y i â€‹ âˆ’Î¼ i â€‹ (s,a)) 2 â€‹ + 2 1 â€‹ logÏƒ i 2 â€‹ (s,a)+constant.
Minimizing this loss encourages the network not only to fit the mean correctly but also to predict a variance that reflects the true uncertainty.

Possibility Weight Update:
Use the variance to update the possibility weights. For each network 
ğ‘–
i, compute the possibility 
ğ‘ ğ‘– ( ğ‘  , ğ‘) p i â€‹
 (s,a) as:

ğ‘ ğ‘– ( ğ‘  , ğ‘) = exp â¡ ( âˆ’ ğœ† â€‰ ğœ ğ‘– 2 ( ğ‘  , ğ‘)) . p i â€‹
 (s,a)=exp(âˆ’Î»Ïƒ i 2 â€‹ (s,a)).
You could combine this with other uncertainty measures (e.g., loss-based likelihoods) multiplicatively or by incorporating it into a combined update:

ğ‘ ğ‘– ( ğ‘  , ğ‘) = ğ‘ ğ‘– â€‰ ğ‘’ âˆ’ ğœ† â€‰ ğœ ğ‘– 2 ( ğ‘  , ğ‘) sup â¡ ğ‘— { ğ‘ ğ‘— â€‰ ğ‘’ âˆ’ ğœ† â€‰ ğœ ğ‘— 2 ( ğ‘  , ğ‘) } , p i â€‹ (s,a)= sup j â€‹ {p j â€‹ e âˆ’Î»Ïƒ j 2 â€‹ (s,a) } p i â€‹ e âˆ’Î»Ïƒ i 2 â€‹ (s,a)
 
â€‹
 ,
where the normalization is performed over the ensemble to ensure that at least one network retains a possibility of 1.

Ensemble Aggregation for Action Selection:
Once you have possibility weights that incorporate variance, the aggregated Q-value for action 
ğ‘
a is:

ğ‘„
ens ( ğ‘  , ğ‘) = âˆ‘ ğ‘– = 1 ğ‘ ğ‘ ğ‘– ğ›¼ ( ğ‘  , ğ‘) â€‰ ğœ‡ ğ‘– ( ğ‘  , ğ‘) âˆ‘ ğ‘– = 1 ğ‘ ğ‘ ğ‘– ğ›¼ ( ğ‘  , ğ‘) . Q ens â€‹
 (s,a)= âˆ‘ i=1 N â€‹
 p i Î± â€‹ (s,a) âˆ‘ i=1 N â€‹ p i Î± â€‹ (s,a)Î¼ i â€‹ (s,a) â€‹
 .
Here, 
ğ›¼
Î± is the same sharpness parameter discussed before. By using the mean predictions 
ğœ‡ ğ‘– ( ğ‘  , ğ‘) Î¼ i â€‹ (s,a) (instead of the raw ğ‘„ ğ‘–
Q 
i
â€‹
  values) and incorporating the variance in the possibility weights, the ensemble action selection becomes more robust to individual network uncertainty.

3. Theoretical Rationale
The idea behind using a meanâ€“var network in this context is twofold:

Uncertainty-Aware Averaging:
Instead of solely relying on the mean Q-value, the network also reports its confidence. A network that is highly uncertain (large variance) is less trusted in the aggregation process. This is analogous to Bayesian model averaging where models with high posterior uncertainty have less influence on the final predictive mean.

Exploration vs. Exploitation Trade-off:
When several ensemble members disagree (which is often the case in poorly explored states), the variances will tend to be higher, leading to lower possibility weights. This can trigger the agent to explore rather than exploit. As more data is gathered, variances may reduce, thereby increasing the possibility weights for the better-performing networks. This dynamic, built into the architecture, helps balance exploration and exploitation.

4. Practical Example
Suppose we have an ensemble of 3 meanâ€“var networks. For a given stateâ€“action pair 
( ğ‘  , ğ‘)
(s,a), assume their outputs are:

ğœ‡ 1 ( ğ‘  , ğ‘) = 5.0 , ğœ 1 2 ( ğ‘  , ğ‘) = 1.0 , ğœ‡ 2 ( ğ‘  , ğ‘) = 4.8 , ğœ 2 2 ( ğ‘  , ğ‘) = 2.0 , ğœ‡ 3 ( ğ‘  , ğ‘) = 5.2 , ğœ 3 2 ( ğ‘  ,
ğ‘
)
=
0.5.
Î¼ 
1
â€‹
 (s,a)
Î¼ 
2
â€‹
 (s,a)
Î¼ 
3
â€‹
 (s,a)
â€‹
  
=5.0,Ïƒ 
1
2
â€‹
 (s,a)=1.0,
=4.8,Ïƒ 
2
2
â€‹
 (s,a)=2.0,
=5.2,Ïƒ 
3
2
â€‹
 (s,a)=0.5.
â€‹
 
Setting 
ğœ†
=
1
Î»=1 and 
ğ›¼
=
1
Î±=1 for simplicity, the possibility weights would be computed as

ğ‘
1
(
ğ‘ 
,
ğ‘
)
=
exp
â¡
(
âˆ’
1
â‹…
1.0
)
â‰ˆ
0.3679
,
ğ‘
2
(
ğ‘ 
,
ğ‘
)
=
exp
â¡
(
âˆ’
1
â‹…
2.0
)
â‰ˆ
0.1353
,
ğ‘
3
(
ğ‘ 
,
ğ‘
)
=
exp
â¡
(
âˆ’
1
â‹…
0.5
)
â‰ˆ
0.6065.
p 
1
â€‹
 (s,a)
p 
2
â€‹
 (s,a)
p 
3
â€‹
 (s,a)
â€‹
  
=exp(âˆ’1â‹…1.0)â‰ˆ0.3679,
=exp(âˆ’1â‹…2.0)â‰ˆ0.1353,
=exp(âˆ’1â‹…0.5)â‰ˆ0.6065.
â€‹
 
The normalized possibility weights (if desired) would be

ğ‘
~
ğ‘–
(
ğ‘ 
,
ğ‘
)
=
ğ‘
ğ‘–
(
ğ‘ 
,
ğ‘
)
ğ‘
1
(
ğ‘ 
,
ğ‘
)
+
ğ‘
2
(
ğ‘ 
,
ğ‘
)
+
ğ‘
3
(
ğ‘ 
,
ğ‘
)
,
p
~
â€‹
  
i
â€‹
 (s,a)= 
p 
1
â€‹
 (s,a)+p 
2
â€‹
 (s,a)+p 
3
â€‹
 (s,a)
p 
i
â€‹
 (s,a)
â€‹
 ,
with

ğ‘
~
1
(
ğ‘ 
,
ğ‘
)
â‰ˆ
0.3679
1.1097
â‰ˆ
0.3314
,
ğ‘
~
2
(
ğ‘ 
,
ğ‘
)
â‰ˆ
0.1220
,
ğ‘
~
3
(
ğ‘ 
,
ğ‘
)
â‰ˆ
0.5466.
p
~
â€‹
  
1
â€‹
 (s,a)â‰ˆ 
1.1097
0.3679
â€‹
 â‰ˆ0.3314, 
p
~
â€‹
  
2
â€‹
 (s,a)â‰ˆ0.1220, 
p
~
â€‹
  
3
â€‹
 (s,a)â‰ˆ0.5466.
Then the possibilistic predictive mean Q-value is

ğ‘„
ens
(
ğ‘ 
,
ğ‘
)
â‰ˆ
0.3314
Ã—
5.0
+
0.1220
Ã—
4.8
+
0.5466
Ã—
5.2
â‰ˆ
1.657
+
0.5856
+
2.8433
â‰ˆ
5.085.
Q 
ens
â€‹
 (s,a)â‰ˆ0.3314Ã—5.0+0.1220Ã—4.8+0.5466Ã—5.2â‰ˆ1.657+0.5856+2.8433â‰ˆ5.085.
This weighted average gives more influence to network 3 because it has both a high mean and a low variance.

Summary
By augmenting each Q-network to predict both a mean and a variance, we can use the variance as an inverse measure of certainty, forming possibility weights according to

ğ‘
ğ‘–
(
ğ‘ 
,
ğ‘
)
=
exp
â¡
(
âˆ’
ğœ†
â€‰
ğœ
ğ‘–
2
(
ğ‘ 
,
ğ‘
)
)
.
p 
i
â€‹
 (s,a)=exp(âˆ’Î»Ïƒ 
i
2
â€‹
 (s,a)).
The ensembleâ€™s aggregated Q-value is then computed as

ğ‘„
ens
ğ›¼
(
ğ‘ 
,
ğ‘
)
=
âˆ‘
ğ‘–
=
1
ğ‘
ğ‘
ğ‘–
ğ›¼
(
ğ‘ 
,
ğ‘
)
â€‰
ğœ‡
ğ‘–
(
ğ‘ 
,
ğ‘
)
âˆ‘
ğ‘–
=
1
ğ‘
ğ‘
ğ‘–
ğ›¼
(
ğ‘ 
,
ğ‘
)
,
Q 
ens
Î±
â€‹
 (s,a)= 
âˆ‘ 
i=1
N
â€‹
 p 
i
Î±
â€‹
 (s,a)
âˆ‘ 
i=1
N
â€‹
 p 
i
Î±
â€‹
 (s,a)Î¼ 
i
â€‹
 (s,a)
â€‹
 ,
where 
ğ›¼
Î± adjusts the sharpness of the weighting.

This approach integrates uncertainty directly into the ensemble framework, ensuring that networks which are less certain about their predictions contribute less to the final decision. The method harmonizes with the overall possibilistic Q-ensemble framework and provides a principled way to balance exploration and exploitation.
