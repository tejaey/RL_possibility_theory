{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fc043b-442b-4e67-9c0a-4c5f2b675f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "MAX_EPISODES = 500\n",
    "MAX_STEPS = 200           \n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99               \n",
    "LR = 1e-3                  \n",
    "EPS_START = 1.0            \n",
    "EPS_END = 0.01            \n",
    "EPS_DECAY = 0.995         \n",
    "TARGET_UPDATE_FREQ = 30     \n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP that takes the state as input and outputs Q-values for each action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer to store tuples of (state, action, reward, next_state, done).\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "                np.array(actions, dtype=np.int64),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.uint8))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def select_action(state, q_network, epsilon, env):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_t)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "def compute_td_loss(batch, q_network, target_network, optimizer):\n",
    "    \"\"\"\n",
    "    Compute the temporal difference loss using a minibatch.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    states_t = torch.FloatTensor(states)\n",
    "    actions_t = torch.LongTensor(actions).unsqueeze(-1)\n",
    "    rewards_t = torch.FloatTensor(rewards).unsqueeze(-1)\n",
    "    next_states_t = torch.FloatTensor(next_states)\n",
    "    dones_t = torch.FloatTensor(dones).unsqueeze(-1)  # 0 or 1\n",
    "\n",
    "    # Get current Q-values\n",
    "    current_q_values = q_network(states_t).gather(1, actions_t)\n",
    "\n",
    "    # Get next Q-values from the target network\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states_t).max(dim=1, keepdim=True)[0]\n",
    "\n",
    "    # If done, next_q_values should be 0\n",
    "    expected_q_values = rewards_t + GAMMA * next_q_values * (1 - dones_t)\n",
    "\n",
    "    loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27614a38-5f56-4fc5-a5c1-beea3f8f75a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 2, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 3, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 4, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 5, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 6, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 7, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 8, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 9, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 10, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 11, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 12, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 13, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 14, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 15, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 16, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 17, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 18, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 19, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 20, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 21, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 22, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 23, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 24, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 25, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 26, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 27, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 28, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 29, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 30, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 31, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 32, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 33, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 34, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 35, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 36, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 37, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 38, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 39, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 40, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 41, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 42, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 43, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 44, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 45, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 46, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 47, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 48, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 49, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 50, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 51, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 52, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 53, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 54, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 55, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 56, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 57, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 58, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 59, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 60, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 61, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 62, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 63, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 64, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 65, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 66, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 67, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 68, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 69, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 70, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 71, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 72, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 73, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 74, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 75, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 76, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 77, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 78, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 79, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 80, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 81, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 82, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 83, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 84, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 85, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 86, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 87, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 88, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 89, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 90, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 91, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 92, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 93, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 94, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 95, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 96, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 97, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 98, Reward: 14.0, Epsilon: 0.010\n",
      "Episode 99, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 100, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 101, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 102, Reward: 13.0, Epsilon: 0.010\n",
      "Episode 103, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 104, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 105, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 106, Reward: 16.0, Epsilon: 0.010\n",
      "Episode 107, Reward: 15.0, Epsilon: 0.010\n",
      "Episode 108, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 109, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 110, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 111, Reward: 21.0, Epsilon: 0.010\n",
      "Episode 112, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 113, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 114, Reward: 18.0, Epsilon: 0.010\n",
      "Episode 115, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 116, Reward: 17.0, Epsilon: 0.010\n",
      "Episode 117, Reward: 14.0, Epsilon: 0.010\n",
      "Episode 118, Reward: 15.0, Epsilon: 0.010\n",
      "Episode 119, Reward: 18.0, Epsilon: 0.010\n",
      "Episode 120, Reward: 15.0, Epsilon: 0.010\n",
      "Episode 121, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 122, Reward: 13.0, Epsilon: 0.010\n",
      "Episode 123, Reward: 15.0, Epsilon: 0.010\n",
      "Episode 124, Reward: 18.0, Epsilon: 0.010\n",
      "Episode 125, Reward: 22.0, Epsilon: 0.010\n",
      "Episode 126, Reward: 22.0, Epsilon: 0.010\n",
      "Episode 127, Reward: 103.0, Epsilon: 0.010\n",
      "Episode 128, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 129, Reward: 91.0, Epsilon: 0.010\n",
      "Episode 130, Reward: 106.0, Epsilon: 0.010\n",
      "Episode 131, Reward: 108.0, Epsilon: 0.010\n",
      "Episode 132, Reward: 91.0, Epsilon: 0.010\n",
      "Episode 133, Reward: 114.0, Epsilon: 0.010\n",
      "Episode 134, Reward: 102.0, Epsilon: 0.010\n",
      "Episode 135, Reward: 103.0, Epsilon: 0.010\n",
      "Episode 136, Reward: 93.0, Epsilon: 0.010\n",
      "Episode 137, Reward: 35.0, Epsilon: 0.010\n",
      "Episode 138, Reward: 64.0, Epsilon: 0.010\n",
      "Episode 139, Reward: 74.0, Epsilon: 0.010\n",
      "Episode 140, Reward: 63.0, Epsilon: 0.010\n",
      "Episode 141, Reward: 95.0, Epsilon: 0.010\n",
      "Episode 142, Reward: 98.0, Epsilon: 0.010\n",
      "Episode 143, Reward: 90.0, Epsilon: 0.010\n",
      "Episode 144, Reward: 100.0, Epsilon: 0.010\n",
      "Episode 145, Reward: 107.0, Epsilon: 0.010\n",
      "Episode 146, Reward: 74.0, Epsilon: 0.010\n",
      "Episode 147, Reward: 71.0, Epsilon: 0.010\n",
      "Episode 148, Reward: 116.0, Epsilon: 0.010\n",
      "Episode 149, Reward: 116.0, Epsilon: 0.010\n",
      "Episode 150, Reward: 106.0, Epsilon: 0.010\n",
      "Episode 151, Reward: 129.0, Epsilon: 0.010\n",
      "Episode 152, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 153, Reward: 145.0, Epsilon: 0.010\n",
      "Episode 154, Reward: 136.0, Epsilon: 0.010\n",
      "Episode 155, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 156, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 157, Reward: 150.0, Epsilon: 0.010\n",
      "Episode 158, Reward: 150.0, Epsilon: 0.010\n",
      "Episode 159, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 160, Reward: 135.0, Epsilon: 0.010\n",
      "Episode 161, Reward: 128.0, Epsilon: 0.010\n",
      "Episode 162, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 163, Reward: 107.0, Epsilon: 0.010\n",
      "Episode 164, Reward: 143.0, Epsilon: 0.010\n",
      "Episode 165, Reward: 131.0, Epsilon: 0.010\n",
      "Episode 166, Reward: 135.0, Epsilon: 0.010\n",
      "Episode 167, Reward: 145.0, Epsilon: 0.010\n",
      "Episode 168, Reward: 147.0, Epsilon: 0.010\n",
      "Episode 169, Reward: 144.0, Epsilon: 0.010\n",
      "Episode 170, Reward: 135.0, Epsilon: 0.010\n",
      "Episode 171, Reward: 132.0, Epsilon: 0.010\n",
      "Episode 172, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 173, Reward: 143.0, Epsilon: 0.010\n",
      "Episode 174, Reward: 127.0, Epsilon: 0.010\n",
      "Episode 175, Reward: 156.0, Epsilon: 0.010\n",
      "Episode 176, Reward: 133.0, Epsilon: 0.010\n",
      "Episode 177, Reward: 130.0, Epsilon: 0.010\n",
      "Episode 178, Reward: 147.0, Epsilon: 0.010\n",
      "Episode 179, Reward: 143.0, Epsilon: 0.010\n",
      "Episode 180, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 181, Reward: 166.0, Epsilon: 0.010\n",
      "Episode 182, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 183, Reward: 141.0, Epsilon: 0.010\n",
      "Episode 184, Reward: 135.0, Epsilon: 0.010\n",
      "Episode 185, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 186, Reward: 154.0, Epsilon: 0.010\n",
      "Episode 187, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 188, Reward: 143.0, Epsilon: 0.010\n",
      "Episode 189, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 190, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 191, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 192, Reward: 122.0, Epsilon: 0.010\n",
      "Episode 193, Reward: 153.0, Epsilon: 0.010\n",
      "Episode 194, Reward: 144.0, Epsilon: 0.010\n",
      "Episode 195, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 196, Reward: 136.0, Epsilon: 0.010\n",
      "Episode 197, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 198, Reward: 143.0, Epsilon: 0.010\n",
      "Episode 199, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 200, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 201, Reward: 128.0, Epsilon: 0.010\n",
      "Episode 202, Reward: 159.0, Epsilon: 0.010\n",
      "Episode 203, Reward: 154.0, Epsilon: 0.010\n",
      "Episode 204, Reward: 145.0, Epsilon: 0.010\n",
      "Episode 205, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 206, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 207, Reward: 129.0, Epsilon: 0.010\n",
      "Episode 208, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 209, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 210, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 211, Reward: 136.0, Epsilon: 0.010\n",
      "Episode 212, Reward: 131.0, Epsilon: 0.010\n",
      "Episode 213, Reward: 131.0, Epsilon: 0.010\n",
      "Episode 214, Reward: 127.0, Epsilon: 0.010\n",
      "Episode 215, Reward: 124.0, Epsilon: 0.010\n",
      "Episode 216, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 217, Reward: 133.0, Epsilon: 0.010\n",
      "Episode 218, Reward: 136.0, Epsilon: 0.010\n",
      "Episode 219, Reward: 130.0, Epsilon: 0.010\n",
      "Episode 220, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 221, Reward: 155.0, Epsilon: 0.010\n",
      "Episode 222, Reward: 159.0, Epsilon: 0.010\n",
      "Episode 223, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 224, Reward: 135.0, Epsilon: 0.010\n",
      "Episode 225, Reward: 148.0, Epsilon: 0.010\n",
      "Episode 226, Reward: 150.0, Epsilon: 0.010\n",
      "Episode 227, Reward: 161.0, Epsilon: 0.010\n",
      "Episode 228, Reward: 168.0, Epsilon: 0.010\n",
      "Episode 229, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 230, Reward: 145.0, Epsilon: 0.010\n",
      "Episode 231, Reward: 168.0, Epsilon: 0.010\n",
      "Episode 232, Reward: 137.0, Epsilon: 0.010\n",
      "Episode 233, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 234, Reward: 158.0, Epsilon: 0.010\n",
      "Episode 235, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 236, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 237, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 238, Reward: 132.0, Epsilon: 0.010\n",
      "Episode 239, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 240, Reward: 162.0, Epsilon: 0.010\n",
      "Episode 241, Reward: 42.0, Epsilon: 0.010\n",
      "Episode 242, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 243, Reward: 181.0, Epsilon: 0.010\n",
      "Episode 244, Reward: 186.0, Epsilon: 0.010\n",
      "Episode 245, Reward: 185.0, Epsilon: 0.010\n",
      "Episode 246, Reward: 153.0, Epsilon: 0.010\n",
      "Episode 247, Reward: 183.0, Epsilon: 0.010\n",
      "Episode 248, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 249, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 250, Reward: 184.0, Epsilon: 0.010\n",
      "Episode 251, Reward: 68.0, Epsilon: 0.010\n",
      "Episode 252, Reward: 104.0, Epsilon: 0.010\n",
      "Episode 253, Reward: 41.0, Epsilon: 0.010\n",
      "Episode 254, Reward: 67.0, Epsilon: 0.010\n",
      "Episode 255, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 256, Reward: 166.0, Epsilon: 0.010\n",
      "Episode 257, Reward: 197.0, Epsilon: 0.010\n",
      "Episode 258, Reward: 18.0, Epsilon: 0.010\n",
      "Episode 259, Reward: 154.0, Epsilon: 0.010\n",
      "Episode 260, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 261, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 262, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 263, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 264, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 265, Reward: 130.0, Epsilon: 0.010\n",
      "Episode 266, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 267, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 268, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 269, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 270, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 271, Reward: 160.0, Epsilon: 0.010\n",
      "Episode 272, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 273, Reward: 126.0, Epsilon: 0.010\n",
      "Episode 274, Reward: 168.0, Epsilon: 0.010\n",
      "Episode 275, Reward: 116.0, Epsilon: 0.010\n",
      "Episode 276, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 277, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 278, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 279, Reward: 155.0, Epsilon: 0.010\n",
      "Episode 280, Reward: 127.0, Epsilon: 0.010\n",
      "Episode 281, Reward: 114.0, Epsilon: 0.010\n",
      "Episode 282, Reward: 121.0, Epsilon: 0.010\n",
      "Episode 283, Reward: 119.0, Epsilon: 0.010\n",
      "Episode 284, Reward: 105.0, Epsilon: 0.010\n",
      "Episode 285, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 286, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 287, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 288, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 289, Reward: 198.0, Epsilon: 0.010\n",
      "Episode 290, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 291, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 292, Reward: 148.0, Epsilon: 0.010\n",
      "Episode 293, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 294, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 295, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 296, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 297, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 298, Reward: 174.0, Epsilon: 0.010\n",
      "Episode 299, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 300, Reward: 169.0, Epsilon: 0.010\n",
      "Episode 301, Reward: 67.0, Epsilon: 0.010\n",
      "Episode 302, Reward: 99.0, Epsilon: 0.010\n",
      "Episode 303, Reward: 103.0, Epsilon: 0.010\n",
      "Episode 304, Reward: 102.0, Epsilon: 0.010\n",
      "Episode 305, Reward: 105.0, Epsilon: 0.010\n",
      "Episode 306, Reward: 93.0, Epsilon: 0.010\n",
      "Episode 307, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 308, Reward: 112.0, Epsilon: 0.010\n",
      "Episode 309, Reward: 112.0, Epsilon: 0.010\n",
      "Episode 310, Reward: 115.0, Epsilon: 0.010\n",
      "Episode 311, Reward: 112.0, Epsilon: 0.010\n",
      "Episode 312, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 313, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 314, Reward: 105.0, Epsilon: 0.010\n",
      "Episode 315, Reward: 128.0, Epsilon: 0.010\n",
      "Episode 316, Reward: 127.0, Epsilon: 0.010\n",
      "Episode 317, Reward: 111.0, Epsilon: 0.010\n",
      "Episode 318, Reward: 101.0, Epsilon: 0.010\n",
      "Episode 319, Reward: 110.0, Epsilon: 0.010\n",
      "Episode 320, Reward: 98.0, Epsilon: 0.010\n",
      "Episode 321, Reward: 127.0, Epsilon: 0.010\n",
      "Episode 322, Reward: 115.0, Epsilon: 0.010\n",
      "Episode 323, Reward: 113.0, Epsilon: 0.010\n",
      "Episode 324, Reward: 111.0, Epsilon: 0.010\n",
      "Episode 325, Reward: 93.0, Epsilon: 0.010\n",
      "Episode 326, Reward: 113.0, Epsilon: 0.010\n",
      "Episode 327, Reward: 148.0, Epsilon: 0.010\n",
      "Episode 328, Reward: 110.0, Epsilon: 0.010\n",
      "Episode 329, Reward: 127.0, Epsilon: 0.010\n",
      "Episode 330, Reward: 111.0, Epsilon: 0.010\n",
      "Episode 331, Reward: 74.0, Epsilon: 0.010\n",
      "Episode 332, Reward: 97.0, Epsilon: 0.010\n",
      "Episode 333, Reward: 86.0, Epsilon: 0.010\n",
      "Episode 334, Reward: 103.0, Epsilon: 0.010\n",
      "Episode 335, Reward: 73.0, Epsilon: 0.010\n",
      "Episode 336, Reward: 85.0, Epsilon: 0.010\n",
      "Episode 337, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 338, Reward: 59.0, Epsilon: 0.010\n",
      "Episode 339, Reward: 21.0, Epsilon: 0.010\n",
      "Episode 340, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 341, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 342, Reward: 19.0, Epsilon: 0.010\n",
      "Episode 343, Reward: 23.0, Epsilon: 0.010\n",
      "Episode 344, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 345, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 346, Reward: 27.0, Epsilon: 0.010\n",
      "Episode 347, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 348, Reward: 32.0, Epsilon: 0.010\n",
      "Episode 349, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 350, Reward: 56.0, Epsilon: 0.010\n",
      "Episode 351, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 352, Reward: 18.0, Epsilon: 0.010\n",
      "Episode 353, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 354, Reward: 15.0, Epsilon: 0.010\n",
      "Episode 355, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 356, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 357, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 358, Reward: 47.0, Epsilon: 0.010\n",
      "Episode 359, Reward: 36.0, Epsilon: 0.010\n",
      "Episode 360, Reward: 40.0, Epsilon: 0.010\n",
      "Episode 361, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 362, Reward: 87.0, Epsilon: 0.010\n",
      "Episode 363, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 364, Reward: 126.0, Epsilon: 0.010\n",
      "Episode 365, Reward: 103.0, Epsilon: 0.010\n",
      "Episode 366, Reward: 79.0, Epsilon: 0.010\n",
      "Episode 367, Reward: 58.0, Epsilon: 0.010\n",
      "Episode 368, Reward: 63.0, Epsilon: 0.010\n",
      "Episode 369, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 370, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 371, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 372, Reward: 109.0, Epsilon: 0.010\n",
      "Episode 373, Reward: 98.0, Epsilon: 0.010\n",
      "Episode 374, Reward: 80.0, Epsilon: 0.010\n",
      "Episode 375, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 376, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 377, Reward: 8.0, Epsilon: 0.010\n",
      "Episode 378, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 379, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 380, Reward: 54.0, Epsilon: 0.010\n",
      "Episode 381, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 382, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 383, Reward: 13.0, Epsilon: 0.010\n",
      "Episode 384, Reward: 9.0, Epsilon: 0.010\n",
      "Episode 385, Reward: 11.0, Epsilon: 0.010\n",
      "Episode 386, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 387, Reward: 24.0, Epsilon: 0.010\n",
      "Episode 388, Reward: 10.0, Epsilon: 0.010\n",
      "Episode 389, Reward: 61.0, Epsilon: 0.010\n",
      "Episode 390, Reward: 53.0, Epsilon: 0.010\n",
      "Episode 391, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 392, Reward: 178.0, Epsilon: 0.010\n",
      "Episode 393, Reward: 195.0, Epsilon: 0.010\n",
      "Episode 394, Reward: 158.0, Epsilon: 0.010\n",
      "Episode 395, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 396, Reward: 145.0, Epsilon: 0.010\n",
      "Episode 397, Reward: 112.0, Epsilon: 0.010\n",
      "Episode 398, Reward: 181.0, Epsilon: 0.010\n",
      "Episode 399, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 400, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 401, Reward: 194.0, Epsilon: 0.010\n",
      "Episode 402, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 403, Reward: 167.0, Epsilon: 0.010\n",
      "Episode 404, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 405, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 406, Reward: 135.0, Epsilon: 0.010\n",
      "Episode 407, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 408, Reward: 131.0, Epsilon: 0.010\n",
      "Episode 409, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 410, Reward: 150.0, Epsilon: 0.010\n",
      "Episode 411, Reward: 102.0, Epsilon: 0.010\n",
      "Episode 412, Reward: 112.0, Epsilon: 0.010\n",
      "Episode 413, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 414, Reward: 123.0, Epsilon: 0.010\n",
      "Episode 415, Reward: 179.0, Epsilon: 0.010\n",
      "Episode 416, Reward: 114.0, Epsilon: 0.010\n",
      "Episode 417, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 418, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 419, Reward: 147.0, Epsilon: 0.010\n",
      "Episode 420, Reward: 133.0, Epsilon: 0.010\n",
      "Episode 421, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 422, Reward: 132.0, Epsilon: 0.010\n",
      "Episode 423, Reward: 182.0, Epsilon: 0.010\n",
      "Episode 424, Reward: 137.0, Epsilon: 0.010\n",
      "Episode 425, Reward: 157.0, Epsilon: 0.010\n",
      "Episode 426, Reward: 167.0, Epsilon: 0.010\n",
      "Episode 427, Reward: 151.0, Epsilon: 0.010\n",
      "Episode 428, Reward: 136.0, Epsilon: 0.010\n",
      "Episode 429, Reward: 170.0, Epsilon: 0.010\n",
      "Episode 430, Reward: 175.0, Epsilon: 0.010\n",
      "Episode 431, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 432, Reward: 167.0, Epsilon: 0.010\n",
      "Episode 433, Reward: 159.0, Epsilon: 0.010\n",
      "Episode 434, Reward: 161.0, Epsilon: 0.010\n",
      "Episode 435, Reward: 137.0, Epsilon: 0.010\n",
      "Episode 436, Reward: 141.0, Epsilon: 0.010\n",
      "Episode 437, Reward: 138.0, Epsilon: 0.010\n",
      "Episode 438, Reward: 189.0, Epsilon: 0.010\n",
      "Episode 439, Reward: 182.0, Epsilon: 0.010\n",
      "Episode 440, Reward: 183.0, Epsilon: 0.010\n",
      "Episode 441, Reward: 143.0, Epsilon: 0.010\n",
      "Episode 442, Reward: 132.0, Epsilon: 0.010\n",
      "Episode 443, Reward: 155.0, Epsilon: 0.010\n",
      "Episode 444, Reward: 162.0, Epsilon: 0.010\n",
      "Episode 445, Reward: 140.0, Epsilon: 0.010\n",
      "Episode 446, Reward: 192.0, Epsilon: 0.010\n",
      "Episode 447, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 448, Reward: 156.0, Epsilon: 0.010\n",
      "Episode 449, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 450, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 451, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 452, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 453, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 454, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 455, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 456, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 457, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 458, Reward: 196.0, Epsilon: 0.010\n",
      "Episode 459, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 460, Reward: 165.0, Epsilon: 0.010\n",
      "Episode 461, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 462, Reward: 187.0, Epsilon: 0.010\n",
      "Episode 463, Reward: 193.0, Epsilon: 0.010\n",
      "Episode 464, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 465, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 466, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 467, Reward: 178.0, Epsilon: 0.010\n",
      "Episode 468, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 469, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 470, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 471, Reward: 188.0, Epsilon: 0.010\n",
      "Episode 472, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 473, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 474, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 475, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 476, Reward: 190.0, Epsilon: 0.010\n",
      "Episode 477, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 478, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 479, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 480, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 481, Reward: 167.0, Epsilon: 0.010\n",
      "Episode 482, Reward: 149.0, Epsilon: 0.010\n",
      "Episode 483, Reward: 160.0, Epsilon: 0.010\n",
      "Episode 484, Reward: 133.0, Epsilon: 0.010\n",
      "Episode 485, Reward: 159.0, Epsilon: 0.010\n",
      "Episode 486, Reward: 161.0, Epsilon: 0.010\n",
      "Episode 487, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 488, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 489, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 490, Reward: 147.0, Epsilon: 0.010\n",
      "Episode 491, Reward: 145.0, Epsilon: 0.010\n",
      "Episode 492, Reward: 167.0, Epsilon: 0.010\n",
      "Episode 493, Reward: 168.0, Epsilon: 0.010\n",
      "Episode 494, Reward: 144.0, Epsilon: 0.010\n",
      "Episode 495, Reward: 165.0, Epsilon: 0.010\n",
      "Episode 496, Reward: 137.0, Epsilon: 0.010\n",
      "Episode 497, Reward: 162.0, Epsilon: 0.010\n",
      "Episode 498, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 499, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 500, Reward: 175.0, Epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize Q-network and target network\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "target_network = QNetwork(state_dim, action_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())  # same initial weights\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "epsilon = EPS_START\n",
    "all_rewards = []\n",
    "\n",
    "# Pre-fill replay buffer with random actions\n",
    "obs, info = env.reset()  # new API: reset returns (obs, info)\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "    obs = next_obs\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(MAX_EPISODES):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action(obs, q_network, epsilon, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward += reward\n",
    "        replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # Sample from replay buffer and update network\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        loss = compute_td_loss(batch, q_network, target_network, optimizer)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "\n",
    "    # Update target network periodically\n",
    "    if (episode + 1) % TARGET_UPDATE_FREQ == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    all_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode+1}, Reward: {episode_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    # Early stopping if environment is solved\n",
    "    # CartPole-v1 is considered solved at average reward >= 195 over 100 consecutive episodes\n",
    "    if len(all_rewards) >= 100 and np.mean(all_rewards[-100:]) >= 195:\n",
    "        print(f\"Solved in {episode+1} episodes!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072349c-692f-4a2c-919f-ac28efa425ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "MAX_EPISODES = 500\n",
    "MAX_STEPS = 200           \n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99               \n",
    "LR = 1e-3                  \n",
    "EPS_START = 1.0            \n",
    "EPS_END = 0.01            \n",
    "EPS_DECAY = 0.995         \n",
    "TARGET_UPDATE_FREQ = 30     \n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP that takes the state as input and outputs Q-values for each action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer to store tuples of (state, action, reward, next_state, done).\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "                np.array(actions, dtype=np.int64),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.uint8))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def select_action(state, q_network, epsilon, env):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_t)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "def compute_td_loss(batch, q_network, target_network, optimizer):\n",
    "    \"\"\"\n",
    "    Compute the temporal difference loss using a minibatch.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    states_t = torch.FloatTensor(states)\n",
    "    actions_t = torch.LongTensor(actions).unsqueeze(-1)\n",
    "    rewards_t = torch.FloatTensor(rewards).unsqueeze(-1)\n",
    "    next_states_t = torch.FloatTensor(next_states)\n",
    "    dones_t = torch.FloatTensor(dones).unsqueeze(-1)  # 0 or 1\n",
    "\n",
    "    # Get current Q-values\n",
    "    current_q_values = q_network(states_t).gather(1, actions_t)\n",
    "\n",
    "    # Get next Q-values from the target network\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states_t).max(dim=1, keepdim=True)[0]\n",
    "\n",
    "    # If done, next_q_values should be 0\n",
    "    expected_q_values = rewards_t + GAMMA * next_q_values * (1 - dones_t)\n",
    "\n",
    "    loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36003fb-3fe2-4e01-964d-9c58141d386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize Q-network and target network\n",
    "q_network = QNetwork(state_dim, action_dim)\n",
    "target_network = QNetwork(state_dim, action_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())  # same initial weights\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "epsilon = EPS_START\n",
    "all_rewards = []\n",
    "\n",
    "# Pre-fill replay buffer with random actions\n",
    "obs, info = env.reset()  # new API: reset returns (obs, info)\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "    obs = next_obs\n",
    "    if done:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(MAX_EPISODES):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action(obs, q_network, epsilon, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward += reward\n",
    "        replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # Sample from replay buffer and update network\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        loss = compute_td_loss(batch, q_network, target_network, optimizer)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "\n",
    "    # Update target network periodically\n",
    "    if (episode + 1) % TARGET_UPDATE_FREQ == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    all_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode+1}, Reward: {episode_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    # Early stopping if environment is solved\n",
    "    # CartPole-v1 is considered solved at average reward >= 195 over 100 consecutive episodes\n",
    "    if len(all_rewards) >= 100 and np.mean(all_rewards[-100:]) >= 195:\n",
    "        print(f\"Solved in {episode+1} episodes!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
